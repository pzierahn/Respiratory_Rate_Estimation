{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e8feb5a154ad",
   "metadata": {},
   "source": [
    "# Data Gathering\n",
    "\n",
    "This notebook gathers the meta-data for the evaluation. It runs the various respiratory extraction methods."
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import respiration.dataset as repository\n",
    "\n",
    "dataset = repository.from_default()\n",
    "subjects = dataset.get_subjects()\n",
    "subjects"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fbaabf772d852710",
   "metadata": {},
   "source": [
    "# The scenarios in which the subjects were recorded\n",
    "# scenarios = dataset.get_scenarios()\n",
    "# scenarios = [\n",
    "#     '101_natural_lighting',\n",
    "#     # '102_artificial_lighting',\n",
    "#     '103_abrupt_changing_lighting',\n",
    "#     '104_dim_lighting_auto_exposure',\n",
    "#     # '106_green_lighting',\n",
    "#     # '107_infrared_lighting',\n",
    "#     '201_shouldercheck',\n",
    "#     '202_scale_movement',\n",
    "#     '203_translation_movement',\n",
    "#     # '204_writing'\n",
    "# ]\n",
    "scenarios = [\n",
    "    '101_natural_lighting',\n",
    "]\n",
    "scenarios"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_dir = os.path.join(os.getcwd(), '..', 'evaluation', 'signals-2024-04-20')\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ],
   "id": "95937112f7167b53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Extract the ground truth signals",
   "id": "2021f96734a84139"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import respiration.utils as utils\n",
    "\n",
    "ground_truth_signals = []\n",
    "\n",
    "for subject in subjects:\n",
    "    for idx, scenario in enumerate(scenarios):\n",
    "        gt_signal, gt_sampling_rate = dataset.get_ground_truth_rr_signal(subject, scenario)\n",
    "        ground_truth_signals.append({\n",
    "            'subject': subject,\n",
    "            'scenario': scenario,\n",
    "            'signal': gt_signal.tolist(),\n",
    "            'sampling_rate': gt_sampling_rate,\n",
    "        })\n",
    "\n",
    "# Save the ground truth signals as a JSON\n",
    "json_path = os.path.join(results_dir, 'ground_truth.json')\n",
    "utils.write_json(json_path, ground_truth_signals)\n",
    "\n",
    "# Save the ground truth signals as a CSV\n",
    "df_gt = pd.DataFrame(ground_truth_signals)\n",
    "csv_path = os.path.join(results_dir, 'ground_truth.csv')\n",
    "df_gt.to_csv(csv_path, index=False)"
   ],
   "id": "9b898a04297aab64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Conduct experiments",
   "id": "e47aef2d563c7065"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "parameters = {\n",
    "    'quality_level': 0.1,\n",
    "    'quality_level_rv': 0.05,\n",
    "    'use_cgof': True,\n",
    "}\n",
    "\n",
    "evaluation_metadata = {\n",
    "    'start_time': datetime.now(),\n",
    "    'subjects': subjects,\n",
    "    'scenarios': scenarios,\n",
    "    'parameters': parameters,\n",
    "}"
   ],
   "id": "d6e2ab154d5334d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.roi as roi\n",
    "\n",
    "yolo = roi.YOLO()\n",
    "\n",
    "\n",
    "def get_rois(frame: np.ndarray) -> list[tuple[np.ndarray, str]]:\n",
    "    \"\"\"\n",
    "    Get the regions of interest (ROIs) for the given frame\n",
    "    :param frame: The frame to get the ROIs from\n",
    "    :return: A list of tuples containing the ROI and the name of the ROI\n",
    "    \"\"\"\n",
    "\n",
    "    regions = [\n",
    "        # ROI for the full frame\n",
    "        ((0, 0, frame.shape[1], frame.shape[0]), 'full')\n",
    "    ]\n",
    "\n",
    "    # Calculate the region of interest (ROI) based on the face\n",
    "    faces = roi.detect_faces(frame)\n",
    "    if len(faces) == 1:\n",
    "        chest_roi = roi.roi_from_face(faces[0])\n",
    "        regions.append((chest_roi, 'chest'))\n",
    "\n",
    "    # Use the detected person to create a mask\n",
    "    persons = yolo.detect_classes(frame, clazz='person')\n",
    "    if len(persons) == 1:\n",
    "        regions.append((persons[0], 'person'))\n",
    "\n",
    "    return regions"
   ],
   "id": "ee5c3a836fc3fe7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2aba3e680aa178da",
   "metadata": {},
   "source": [
    "from respiration.extractor import pixel_intensity, optical_flow\n",
    "from respiration.extractor.mtts_can import load_model, calculate_cutoff, preprocess_video_frames\n",
    "\n",
    "extracted_signals = []\n",
    "\n",
    "frame_depth = 10\n",
    "mtts_model = load_model(frame_depth=frame_depth)\n",
    "\n",
    "for subject in subjects:\n",
    "    for idx, scenario in enumerate(scenarios):\n",
    "        print(f'Processing {subject} - {scenario}')\n",
    "\n",
    "        frames, params = dataset.get_video_bgr(subject, scenario, False)\n",
    "\n",
    "        frames_gray = utils.convert_to_gray(frames)\n",
    "\n",
    "        rois = get_rois(frames_gray[0])\n",
    "\n",
    "        for region in rois:\n",
    "            roi_area, roi_name = region\n",
    "\n",
    "            #\n",
    "            # Calculate the average pixel intensity\n",
    "            #\n",
    "\n",
    "            pi_start = datetime.now()\n",
    "            pi_signal = pixel_intensity.average_pixel_intensity(frames_gray, roi=roi_area)\n",
    "            extracted_signals.append({\n",
    "                'subject': subject,\n",
    "                'scenario': scenario,\n",
    "                'method': 'pixel_intensity',\n",
    "                'roi': roi_name,\n",
    "                'roi_area': list(roi_area),\n",
    "                'execution_time': datetime.now() - pi_start,\n",
    "                'sampling_rate': params.fps,\n",
    "                'signal': pi_signal.tolist(),\n",
    "            })\n",
    "\n",
    "            #\n",
    "            # Calculate the optical flow\n",
    "            #\n",
    "\n",
    "            of_cgof_start = datetime.now()\n",
    "            of_signal_raw = optical_flow.extract_signal(\n",
    "                frames_gray,\n",
    "                roi=roi_area,\n",
    "                quality_level=parameters['quality_level'],\n",
    "                quality_level_rv=parameters['quality_level_rv'],\n",
    "                use_cgof=parameters['use_cgof'],\n",
    "            )\n",
    "\n",
    "            extracted_signals.append({\n",
    "                'subject': subject,\n",
    "                'scenario': scenario,\n",
    "                'method': 'optical_flow',\n",
    "                'roi': roi_name,\n",
    "                'roi_area': list(roi_area),\n",
    "                'execution_time': datetime.now() - of_cgof_start,\n",
    "                'sampling_rate': params.fps,\n",
    "                'signal': of_signal_raw.tolist(),\n",
    "            })\n",
    "\n",
    "        #\n",
    "        # Calculate the MTTS-CAN signal\n",
    "        #\n",
    "\n",
    "        mtts_start = datetime.now()\n",
    "        resized, normalized = preprocess_video_frames(frames)\n",
    "        cutoff = calculate_cutoff(resized.shape[0], frame_depth)\n",
    "\n",
    "        resized = resized[:cutoff]\n",
    "        normalized = normalized[:cutoff]\n",
    "\n",
    "        mtts_raw = mtts_model.predict(\n",
    "            (resized, normalized),\n",
    "            batch_size=100\n",
    "        )\n",
    "        extracted_signals.append({\n",
    "            'subject': subject,\n",
    "            'scenario': scenario,\n",
    "            'method': 'mtts_can',\n",
    "            'roi': 'full',\n",
    "            'roi_area': [0, 0, frames.shape[2], frames.shape[1]],\n",
    "            'execution_time': datetime.now() - mtts_start,\n",
    "            'sampling_rate': params.fps,\n",
    "            'signal': np.cumsum(mtts_raw[1]).tolist(),\n",
    "        })\n",
    "\n",
    "        # Garbage collect the frames\n",
    "        del frames"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluation_metadata['end_time'] = datetime.now()",
   "id": "4361f0aa4df2465",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(extracted_signals)\n",
    "df.head()"
   ],
   "id": "e94ff7cb59c2a3ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the extracted_signals as a JSON\n",
    "json_path = os.path.join(results_dir, 'predictions.json')\n",
    "utils.write_json(json_path, extracted_signals)\n",
    "\n",
    "# Save the evaluation dataframe\n",
    "csv_path = os.path.join(results_dir, 'predictions.csv')\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Save the hyperparameters as prettified json\n",
    "json_path = os.path.join(results_dir, 'parameters.json')\n",
    "utils.write_json(json_path, evaluation_metadata)"
   ],
   "id": "3806544e615aa349",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
