{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Exploration\n",
    "\n",
    "This notebook demonstrates how to read a video file, extract frames, and display them using OpenCV and Matplotlib. It also shows how to detect faces in a frame using the Viola-Jones algorithm."
   ],
   "id": "349ba13172cd3774"
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "\n",
    "input_video_path = '/Volumes/Patrick/Datasets/VitalCamSet/Proband16/101_natural_lighting/Logitech HD Pro Webcam C920.avi'\n",
    "video = cv2.VideoCapture(input_video_path)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4a4d6ee80f41e1d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Video properties",
   "id": "77f07b6f92634c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the video properties\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "duration = frame_count / fps\n",
    "\n",
    "print(f'FPS: {fps}')\n",
    "print(f'Frame count: {frame_count}')\n",
    "print(f'Frame: {frame_width}x{frame_height}')\n",
    "print(f'Duration: {duration:.2f} seconds')"
   ],
   "id": "d7894034d041f859",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Display random frames",
   "id": "e31a2710464b2da4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# Seed the random number generator for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "frame_inx1 = 20\n",
    "frame_inx2 = 21\n",
    "# frame_inx1 = random.randint(0, frame_count)\n",
    "# frame_inx2 = random.randint(0, frame_count)\n",
    "\n",
    "video.set(cv2.CAP_PROP_POS_FRAMES, frame_inx1)\n",
    "_, frame1 = video.read()\n",
    "\n",
    "video.set(cv2.CAP_PROP_POS_FRAMES, frame_inx2)\n",
    "_, frame2 = video.read()"
   ],
   "id": "5dfbe5b61af18c57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB)\n",
    "axes[0].imshow(frame1)\n",
    "axes[0].set_title(f'Frame {frame_inx1}')\n",
    "\n",
    "frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
    "axes[1].imshow(frame2)\n",
    "axes[1].set_title(f'Frame {frame_inx2}')"
   ],
   "id": "8e3e0b0729265182",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RAFT Model",
   "id": "2faa2d842bd1e26a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "\n",
    "\n",
    "class InputPadder:\n",
    "    \"\"\" Pads images such that dimensions are divisible by 8 \"\"\"\n",
    "\n",
    "    def __init__(self, dims, mode='sintel'):\n",
    "        self.ht, self.wd = dims[-2:]\n",
    "        pad_ht = (((self.ht // 8) + 1) * 8 - self.ht) % 8\n",
    "        pad_wd = (((self.wd // 8) + 1) * 8 - self.wd) % 8\n",
    "        if mode == 'sintel':\n",
    "            self._pad = [pad_wd // 2, pad_wd - pad_wd // 2, pad_ht // 2, pad_ht - pad_ht // 2]\n",
    "        else:\n",
    "            self._pad = [pad_wd // 2, pad_wd - pad_wd // 2, 0, pad_ht]\n",
    "\n",
    "    def pad(self, *inputs):\n",
    "        return [F.pad(x, self._pad, mode='replicate') for x in inputs]\n",
    "\n",
    "    def unpad(self, x):\n",
    "        ht, wd = x.shape[-2:]\n",
    "        c = [self._pad[2], ht - self._pad[3], self._pad[0], wd - self._pad[1]]\n",
    "        return x[..., c[0]:c[1], c[2]:c[3]]\n",
    "\n",
    "\n",
    "def forward_interpolate(flow):\n",
    "    flow = flow.detach().cpu().numpy()\n",
    "    dx, dy = flow[0], flow[1]\n",
    "\n",
    "    ht, wd = dx.shape\n",
    "    x0, y0 = np.meshgrid(np.arange(wd), np.arange(ht))\n",
    "\n",
    "    x1 = x0 + dx\n",
    "    y1 = y0 + dy\n",
    "\n",
    "    x1 = x1.reshape(-1)\n",
    "    y1 = y1.reshape(-1)\n",
    "    dx = dx.reshape(-1)\n",
    "    dy = dy.reshape(-1)\n",
    "\n",
    "    valid = (x1 > 0) & (x1 < wd) & (y1 > 0) & (y1 < ht)\n",
    "    x1 = x1[valid]\n",
    "    y1 = y1[valid]\n",
    "    dx = dx[valid]\n",
    "    dy = dy[valid]\n",
    "\n",
    "    flow_x = interpolate.griddata(\n",
    "        (x1, y1), dx, (x0, y0), method='nearest', fill_value=0)\n",
    "\n",
    "    flow_y = interpolate.griddata(\n",
    "        (x1, y1), dy, (x0, y0), method='nearest', fill_value=0)\n",
    "\n",
    "    flow = np.stack([flow_x, flow_y], axis=0)\n",
    "    return torch.from_numpy(flow).float()\n",
    "\n",
    "\n",
    "def bilinear_sampler(img, coords, mode='bilinear', mask=False):\n",
    "    \"\"\" Wrapper for grid_sample, uses pixel coordinates \"\"\"\n",
    "    H, W = img.shape[-2:]\n",
    "    xgrid, ygrid = coords.split([1, 1], dim=-1)\n",
    "    xgrid = 2 * xgrid / (W - 1) - 1\n",
    "    ygrid = 2 * ygrid / (H - 1) - 1\n",
    "\n",
    "    grid = torch.cat([xgrid, ygrid], dim=-1)\n",
    "    img = F.grid_sample(img, grid, align_corners=True)\n",
    "\n",
    "    if mask:\n",
    "        mask = (xgrid > -1) & (ygrid > -1) & (xgrid < 1) & (ygrid < 1)\n",
    "        return img, mask.float()\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def coords_grid(batch, ht, wd, device):\n",
    "    coords = torch.meshgrid(torch.arange(ht, device=device), torch.arange(wd, device=device))\n",
    "    coords = torch.stack(coords[::-1], dim=0).float()\n",
    "    return coords[None].repeat(batch, 1, 1, 1)\n",
    "\n",
    "\n",
    "def upflow8(flow, mode='bilinear'):\n",
    "    new_size = (8 * flow.shape[2], 8 * flow.shape[3])\n",
    "    return 8 * F.interpolate(flow, size=new_size, mode=mode, align_corners=True)"
   ],
   "id": "2159fc46c11fa980",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FlowHead(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim=256):\n",
    "        super(FlowHead, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_dim, hidden_dim, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, 2, 3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "\n",
    "class ConvGRU(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, input_dim=192 + 128):\n",
    "        super(ConvGRU, self).__init__()\n",
    "        self.convz = nn.Conv2d(hidden_dim + input_dim, hidden_dim, 3, padding=1)\n",
    "        self.convr = nn.Conv2d(hidden_dim + input_dim, hidden_dim, 3, padding=1)\n",
    "        self.convq = nn.Conv2d(hidden_dim + input_dim, hidden_dim, 3, padding=1)\n",
    "\n",
    "    def forward(self, h, x):\n",
    "        hx = torch.cat([h, x], dim=1)\n",
    "\n",
    "        z = torch.sigmoid(self.convz(hx))\n",
    "        r = torch.sigmoid(self.convr(hx))\n",
    "        q = torch.tanh(self.convq(torch.cat([r * h, x], dim=1)))\n",
    "\n",
    "        h = (1 - z) * h + z * q\n",
    "        return h\n",
    "\n",
    "\n",
    "class SepConvGRU(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, input_dim=192 + 128):\n",
    "        super(SepConvGRU, self).__init__()\n",
    "        self.convz1 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (1, 5), padding=(0, 2))\n",
    "        self.convr1 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (1, 5), padding=(0, 2))\n",
    "        self.convq1 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (1, 5), padding=(0, 2))\n",
    "\n",
    "        self.convz2 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (5, 1), padding=(2, 0))\n",
    "        self.convr2 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (5, 1), padding=(2, 0))\n",
    "        self.convq2 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (5, 1), padding=(2, 0))\n",
    "\n",
    "    def forward(self, h, x):\n",
    "        # horizontal\n",
    "        hx = torch.cat([h, x], dim=1)\n",
    "        z = torch.sigmoid(self.convz1(hx))\n",
    "        r = torch.sigmoid(self.convr1(hx))\n",
    "        q = torch.tanh(self.convq1(torch.cat([r * h, x], dim=1)))\n",
    "        h = (1 - z) * h + z * q\n",
    "\n",
    "        # vertical\n",
    "        hx = torch.cat([h, x], dim=1)\n",
    "        z = torch.sigmoid(self.convz2(hx))\n",
    "        r = torch.sigmoid(self.convr2(hx))\n",
    "        q = torch.tanh(self.convq2(torch.cat([r * h, x], dim=1)))\n",
    "        h = (1 - z) * h + z * q\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "class SmallMotionEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(SmallMotionEncoder, self).__init__()\n",
    "        cor_planes = args.corr_levels * (2 * args.corr_radius + 1) ** 2\n",
    "        self.convc1 = nn.Conv2d(cor_planes, 96, 1, padding=0)\n",
    "        self.convf1 = nn.Conv2d(2, 64, 7, padding=3)\n",
    "        self.convf2 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.conv = nn.Conv2d(128, 80, 3, padding=1)\n",
    "\n",
    "    def forward(self, flow, corr):\n",
    "        cor = F.relu(self.convc1(corr))\n",
    "        flo = F.relu(self.convf1(flow))\n",
    "        flo = F.relu(self.convf2(flo))\n",
    "        cor_flo = torch.cat([cor, flo], dim=1)\n",
    "        out = F.relu(self.conv(cor_flo))\n",
    "        return torch.cat([out, flow], dim=1)\n",
    "\n",
    "\n",
    "class BasicMotionEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(BasicMotionEncoder, self).__init__()\n",
    "        cor_planes = args.corr_levels * (2 * args.corr_radius + 1) ** 2\n",
    "        self.convc1 = nn.Conv2d(cor_planes, 256, 1, padding=0)\n",
    "        self.convc2 = nn.Conv2d(256, 192, 3, padding=1)\n",
    "        self.convf1 = nn.Conv2d(2, 128, 7, padding=3)\n",
    "        self.convf2 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.conv = nn.Conv2d(64 + 192, 128 - 2, 3, padding=1)\n",
    "\n",
    "    def forward(self, flow, corr):\n",
    "        cor = F.relu(self.convc1(corr))\n",
    "        cor = F.relu(self.convc2(cor))\n",
    "        flo = F.relu(self.convf1(flow))\n",
    "        flo = F.relu(self.convf2(flo))\n",
    "\n",
    "        cor_flo = torch.cat([cor, flo], dim=1)\n",
    "        out = F.relu(self.conv(cor_flo))\n",
    "        return torch.cat([out, flow], dim=1)\n",
    "\n",
    "\n",
    "class SmallUpdateBlock(nn.Module):\n",
    "    def __init__(self, args, hidden_dim=96):\n",
    "        super(SmallUpdateBlock, self).__init__()\n",
    "        self.encoder = SmallMotionEncoder(args)\n",
    "        self.gru = ConvGRU(hidden_dim=hidden_dim, input_dim=82 + 64)\n",
    "        self.flow_head = FlowHead(hidden_dim, hidden_dim=128)\n",
    "\n",
    "    def forward(self, net, inp, corr, flow):\n",
    "        motion_features = self.encoder(flow, corr)\n",
    "        inp = torch.cat([inp, motion_features], dim=1)\n",
    "        net = self.gru(net, inp)\n",
    "        delta_flow = self.flow_head(net)\n",
    "\n",
    "        return net, None, delta_flow\n",
    "\n",
    "\n",
    "class BasicUpdateBlock(nn.Module):\n",
    "    def __init__(self, args, hidden_dim=128, input_dim=128):\n",
    "        super(BasicUpdateBlock, self).__init__()\n",
    "        self.args = args\n",
    "        self.encoder = BasicMotionEncoder(args)\n",
    "        self.gru = SepConvGRU(hidden_dim=hidden_dim, input_dim=128 + hidden_dim)\n",
    "        self.flow_head = FlowHead(hidden_dim, hidden_dim=256)\n",
    "\n",
    "        self.mask = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 64 * 9, 1, padding=0))\n",
    "\n",
    "    def forward(self, net, inp, corr, flow, upsample=True):\n",
    "        motion_features = self.encoder(flow, corr)\n",
    "        inp = torch.cat([inp, motion_features], dim=1)\n",
    "\n",
    "        net = self.gru(net, inp)\n",
    "        delta_flow = self.flow_head(net)\n",
    "\n",
    "        # scale mask to balence gradients\n",
    "        mask = .25 * self.mask(net)\n",
    "        return net, mask, delta_flow"
   ],
   "id": "44795f84d5ccee8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, stride=stride)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        num_groups = planes // 8\n",
    "\n",
    "        if norm_fn == 'group':\n",
    "            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n",
    "            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n",
    "            if not stride == 1:\n",
    "                self.norm3 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n",
    "\n",
    "        elif norm_fn == 'batch':\n",
    "            self.norm1 = nn.BatchNorm2d(planes)\n",
    "            self.norm2 = nn.BatchNorm2d(planes)\n",
    "            if not stride == 1:\n",
    "                self.norm3 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        elif norm_fn == 'instance':\n",
    "            self.norm1 = nn.InstanceNorm2d(planes)\n",
    "            self.norm2 = nn.InstanceNorm2d(planes)\n",
    "            if not stride == 1:\n",
    "                self.norm3 = nn.InstanceNorm2d(planes)\n",
    "\n",
    "        elif norm_fn == 'none':\n",
    "            self.norm1 = nn.Sequential()\n",
    "            self.norm2 = nn.Sequential()\n",
    "            if not stride == 1:\n",
    "                self.norm3 = nn.Sequential()\n",
    "\n",
    "        if stride == 1:\n",
    "            self.downsample = None\n",
    "\n",
    "        else:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride), self.norm3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.relu(self.norm1(self.conv1(y)))\n",
    "        y = self.relu(self.norm2(self.conv2(y)))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        return self.relu(x + y)\n",
    "\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes // 4, kernel_size=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(planes // 4, planes // 4, kernel_size=3, padding=1, stride=stride)\n",
    "        self.conv3 = nn.Conv2d(planes // 4, planes, kernel_size=1, padding=0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        num_groups = planes // 8\n",
    "\n",
    "        if norm_fn == 'group':\n",
    "            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes // 4)\n",
    "            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=planes // 4)\n",
    "            self.norm3 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n",
    "            if not stride == 1:\n",
    "                self.norm4 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n",
    "\n",
    "        elif norm_fn == 'batch':\n",
    "            self.norm1 = nn.BatchNorm2d(planes // 4)\n",
    "            self.norm2 = nn.BatchNorm2d(planes // 4)\n",
    "            self.norm3 = nn.BatchNorm2d(planes)\n",
    "            if not stride == 1:\n",
    "                self.norm4 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        elif norm_fn == 'instance':\n",
    "            self.norm1 = nn.InstanceNorm2d(planes // 4)\n",
    "            self.norm2 = nn.InstanceNorm2d(planes // 4)\n",
    "            self.norm3 = nn.InstanceNorm2d(planes)\n",
    "            if not stride == 1:\n",
    "                self.norm4 = nn.InstanceNorm2d(planes)\n",
    "\n",
    "        elif norm_fn == 'none':\n",
    "            self.norm1 = nn.Sequential()\n",
    "            self.norm2 = nn.Sequential()\n",
    "            self.norm3 = nn.Sequential()\n",
    "            if not stride == 1:\n",
    "                self.norm4 = nn.Sequential()\n",
    "\n",
    "        if stride == 1:\n",
    "            self.downsample = None\n",
    "\n",
    "        else:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride), self.norm4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.relu(self.norm1(self.conv1(y)))\n",
    "        y = self.relu(self.norm2(self.conv2(y)))\n",
    "        y = self.relu(self.norm3(self.conv3(y)))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        return self.relu(x + y)\n",
    "\n",
    "\n",
    "class BasicEncoder(nn.Module):\n",
    "    def __init__(self, output_dim=128, norm_fn='batch', dropout=0.0):\n",
    "        super(BasicEncoder, self).__init__()\n",
    "        self.norm_fn = norm_fn\n",
    "\n",
    "        if self.norm_fn == 'group':\n",
    "            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=64)\n",
    "\n",
    "        elif self.norm_fn == 'batch':\n",
    "            self.norm1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        elif self.norm_fn == 'instance':\n",
    "            self.norm1 = nn.InstanceNorm2d(64)\n",
    "\n",
    "        elif self.norm_fn == 'none':\n",
    "            self.norm1 = nn.Sequential()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.in_planes = 64\n",
    "        self.layer1 = self._make_layer(64, stride=1)\n",
    "        self.layer2 = self._make_layer(96, stride=2)\n",
    "        self.layer3 = self._make_layer(128, stride=2)\n",
    "\n",
    "        # output convolution\n",
    "        self.conv2 = nn.Conv2d(128, output_dim, kernel_size=1)\n",
    "\n",
    "        self.dropout = None\n",
    "        if dropout > 0:\n",
    "            self.dropout = nn.Dropout2d(p=dropout)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.InstanceNorm2d, nn.GroupNorm)):\n",
    "                if m.weight is not None:\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, dim, stride=1):\n",
    "        layer1 = ResidualBlock(self.in_planes, dim, self.norm_fn, stride=stride)\n",
    "        layer2 = ResidualBlock(dim, dim, self.norm_fn, stride=1)\n",
    "        layers = (layer1, layer2)\n",
    "\n",
    "        self.in_planes = dim\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # if input is list, combine batch dimension\n",
    "        is_list = isinstance(x, tuple) or isinstance(x, list)\n",
    "        if is_list:\n",
    "            batch_dim = x[0].shape[0]\n",
    "            x = torch.cat(x, dim=0)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        if self.training and self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        if is_list:\n",
    "            x = torch.split(x, [batch_dim, batch_dim], dim=0)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SmallEncoder(nn.Module):\n",
    "    def __init__(self, output_dim=128, norm_fn='batch', dropout=0.0):\n",
    "        super(SmallEncoder, self).__init__()\n",
    "        self.norm_fn = norm_fn\n",
    "\n",
    "        if self.norm_fn == 'group':\n",
    "            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=32)\n",
    "\n",
    "        elif self.norm_fn == 'batch':\n",
    "            self.norm1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        elif self.norm_fn == 'instance':\n",
    "            self.norm1 = nn.InstanceNorm2d(32)\n",
    "\n",
    "        elif self.norm_fn == 'none':\n",
    "            self.norm1 = nn.Sequential()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.in_planes = 32\n",
    "        self.layer1 = self._make_layer(32, stride=1)\n",
    "        self.layer2 = self._make_layer(64, stride=2)\n",
    "        self.layer3 = self._make_layer(96, stride=2)\n",
    "\n",
    "        self.dropout = None\n",
    "        if dropout > 0:\n",
    "            self.dropout = nn.Dropout2d(p=dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(96, output_dim, kernel_size=1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.InstanceNorm2d, nn.GroupNorm)):\n",
    "                if m.weight is not None:\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, dim, stride=1):\n",
    "        layer1 = BottleneckBlock(self.in_planes, dim, self.norm_fn, stride=stride)\n",
    "        layer2 = BottleneckBlock(dim, dim, self.norm_fn, stride=1)\n",
    "        layers = (layer1, layer2)\n",
    "\n",
    "        self.in_planes = dim\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # if input is list, combine batch dimension\n",
    "        is_list = isinstance(x, tuple) or isinstance(x, list)\n",
    "        if is_list:\n",
    "            batch_dim = x[0].shape[0]\n",
    "            x = torch.cat(x, dim=0)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        if self.training and self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        if is_list:\n",
    "            x = torch.split(x, [batch_dim, batch_dim], dim=0)\n",
    "\n",
    "        return x"
   ],
   "id": "57561b183f2de61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    import alt_cuda_corr\n",
    "except:\n",
    "    # alt_cuda_corr is not compiled\n",
    "    pass\n",
    "\n",
    "\n",
    "class CorrBlock:\n",
    "    def __init__(self, fmap1, fmap2, num_levels=4, radius=4):\n",
    "        self.num_levels = num_levels\n",
    "        self.radius = radius\n",
    "        self.corr_pyramid = []\n",
    "\n",
    "        # all pairs correlation\n",
    "        corr = CorrBlock.corr(fmap1, fmap2)\n",
    "\n",
    "        batch, h1, w1, dim, h2, w2 = corr.shape\n",
    "        corr = corr.reshape(batch * h1 * w1, dim, h2, w2)\n",
    "\n",
    "        self.corr_pyramid.append(corr)\n",
    "        for i in range(self.num_levels - 1):\n",
    "            corr = F.avg_pool2d(corr, 2, stride=2)\n",
    "            self.corr_pyramid.append(corr)\n",
    "\n",
    "    def __call__(self, coords):\n",
    "        r = self.radius\n",
    "        coords = coords.permute(0, 2, 3, 1)\n",
    "        batch, h1, w1, _ = coords.shape\n",
    "\n",
    "        out_pyramid = []\n",
    "        for i in range(self.num_levels):\n",
    "            corr = self.corr_pyramid[i]\n",
    "            dx = torch.linspace(-r, r, 2 * r + 1, device=coords.device)\n",
    "            dy = torch.linspace(-r, r, 2 * r + 1, device=coords.device)\n",
    "            delta = torch.stack(torch.meshgrid(dy, dx), axis=-1)\n",
    "\n",
    "            centroid_lvl = coords.reshape(batch * h1 * w1, 1, 1, 2) / 2 ** i\n",
    "            delta_lvl = delta.view(1, 2 * r + 1, 2 * r + 1, 2)\n",
    "            coords_lvl = centroid_lvl + delta_lvl\n",
    "\n",
    "            corr = bilinear_sampler(corr, coords_lvl)\n",
    "            corr = corr.view(batch, h1, w1, -1)\n",
    "            out_pyramid.append(corr)\n",
    "\n",
    "        out = torch.cat(out_pyramid, dim=-1)\n",
    "        return out.permute(0, 3, 1, 2).contiguous().float()\n",
    "\n",
    "    @staticmethod\n",
    "    def corr(fmap1, fmap2):\n",
    "        batch, dim, ht, wd = fmap1.shape\n",
    "        fmap1 = fmap1.view(batch, dim, ht * wd)\n",
    "        fmap2 = fmap2.view(batch, dim, ht * wd)\n",
    "\n",
    "        corr = torch.matmul(fmap1.transpose(1, 2), fmap2)\n",
    "        corr = corr.view(batch, ht, wd, 1, ht, wd)\n",
    "        return corr / torch.sqrt(torch.tensor(dim).float())\n",
    "\n",
    "\n",
    "class AlternateCorrBlock:\n",
    "    def __init__(self, fmap1, fmap2, num_levels=4, radius=4):\n",
    "        self.num_levels = num_levels\n",
    "        self.radius = radius\n",
    "\n",
    "        self.pyramid = [(fmap1, fmap2)]\n",
    "        for i in range(self.num_levels):\n",
    "            fmap1 = F.avg_pool2d(fmap1, 2, stride=2)\n",
    "            fmap2 = F.avg_pool2d(fmap2, 2, stride=2)\n",
    "            self.pyramid.append((fmap1, fmap2))\n",
    "\n",
    "    def __call__(self, coords):\n",
    "        coords = coords.permute(0, 2, 3, 1)\n",
    "        B, H, W, _ = coords.shape\n",
    "        dim = self.pyramid[0][0].shape[1]\n",
    "\n",
    "        corr_list = []\n",
    "        for i in range(self.num_levels):\n",
    "            r = self.radius\n",
    "            fmap1_i = self.pyramid[0][0].permute(0, 2, 3, 1).contiguous()\n",
    "            fmap2_i = self.pyramid[i][1].permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "            coords_i = (coords / 2 ** i).reshape(B, 1, H, W, 2).contiguous()\n",
    "            corr, = alt_cuda_corr.forward(fmap1_i, fmap2_i, coords_i, r)\n",
    "            corr_list.append(corr.squeeze(1))\n",
    "\n",
    "        corr = torch.stack(corr_list, dim=1)\n",
    "        corr = corr.reshape(B, -1, H, W)\n",
    "        return corr / torch.sqrt(torch.tensor(dim).float())"
   ],
   "id": "a688143a8be1a1c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    autocast = torch.cuda.amp.autocast\n",
    "except:\n",
    "    # dummy autocast for PyTorch < 1.6\n",
    "    class autocast:\n",
    "        def __init__(self, enabled):\n",
    "            pass\n",
    "\n",
    "        def __enter__(self):\n",
    "            pass\n",
    "\n",
    "        def __exit__(self, *args):\n",
    "            pass\n",
    "\n",
    "\n",
    "class RAFT(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(RAFT, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        if args.small:\n",
    "            self.hidden_dim = hdim = 96\n",
    "            self.context_dim = cdim = 64\n",
    "            args.corr_levels = 4\n",
    "            args.corr_radius = 3\n",
    "\n",
    "        else:\n",
    "            self.hidden_dim = hdim = 128\n",
    "            self.context_dim = cdim = 128\n",
    "            args.corr_levels = 4\n",
    "            args.corr_radius = 4\n",
    "\n",
    "        if 'dropout' not in self.args:\n",
    "            self.args.dropout = 0\n",
    "\n",
    "        if 'alternate_corr' not in self.args:\n",
    "            self.args.alternate_corr = False\n",
    "\n",
    "        # feature network, context network, and update block\n",
    "        if args.small:\n",
    "            self.fnet = SmallEncoder(output_dim=128, norm_fn='instance', dropout=args.dropout)\n",
    "            self.cnet = SmallEncoder(output_dim=hdim + cdim, norm_fn='none', dropout=args.dropout)\n",
    "            self.update_block = SmallUpdateBlock(self.args, hidden_dim=hdim)\n",
    "\n",
    "        else:\n",
    "            self.fnet = BasicEncoder(output_dim=256, norm_fn='instance', dropout=args.dropout)\n",
    "            self.cnet = BasicEncoder(output_dim=hdim + cdim, norm_fn='batch', dropout=args.dropout)\n",
    "            self.update_block = BasicUpdateBlock(self.args, hidden_dim=hdim)\n",
    "\n",
    "    def freeze_bn(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "\n",
    "    def initialize_flow(self, img):\n",
    "        \"\"\" Flow is represented as difference between two coordinate grids flow = coords1 - coords0\"\"\"\n",
    "        N, C, H, W = img.shape\n",
    "        coords0 = coords_grid(N, H // 8, W // 8, device=img.device)\n",
    "        coords1 = coords_grid(N, H // 8, W // 8, device=img.device)\n",
    "\n",
    "        # optical flow computed as difference: flow = coords1 - coords0\n",
    "        return coords0, coords1\n",
    "\n",
    "    def upsample_flow(self, flow, mask):\n",
    "        \"\"\" Upsample flow field [H/8, W/8, 2] -> [H, W, 2] using convex combination \"\"\"\n",
    "        N, _, H, W = flow.shape\n",
    "        mask = mask.view(N, 1, 9, 8, 8, H, W)\n",
    "        mask = torch.softmax(mask, dim=2)\n",
    "\n",
    "        up_flow = F.unfold(8 * flow, [3, 3], padding=1)\n",
    "        up_flow = up_flow.view(N, 2, 9, 1, 1, H, W)\n",
    "\n",
    "        up_flow = torch.sum(mask * up_flow, dim=2)\n",
    "        up_flow = up_flow.permute(0, 1, 4, 2, 5, 3)\n",
    "        return up_flow.reshape(N, 2, 8 * H, 8 * W)\n",
    "\n",
    "    def forward(self, image1, image2, iters=12, flow_init=None, upsample=True, test_mode=False):\n",
    "        \"\"\" Estimate optical flow between pair of frames \"\"\"\n",
    "\n",
    "        image1 = 2 * (image1 / 255.0) - 1.0\n",
    "        image2 = 2 * (image2 / 255.0) - 1.0\n",
    "\n",
    "        image1 = image1.contiguous()\n",
    "        image2 = image2.contiguous()\n",
    "\n",
    "        hdim = self.hidden_dim\n",
    "        cdim = self.context_dim\n",
    "\n",
    "        # run the feature network\n",
    "        with autocast(enabled=self.args.mixed_precision):\n",
    "            fmap1, fmap2 = self.fnet([image1, image2])\n",
    "\n",
    "        fmap1 = fmap1.float()\n",
    "        fmap2 = fmap2.float()\n",
    "        if self.args.alternate_corr:\n",
    "            corr_fn = AlternateCorrBlock(fmap1, fmap2, radius=self.args.corr_radius)\n",
    "        else:\n",
    "            corr_fn = CorrBlock(fmap1, fmap2, radius=self.args.corr_radius)\n",
    "\n",
    "        # run the context network\n",
    "        with autocast(enabled=self.args.mixed_precision):\n",
    "            cnet = self.cnet(image1)\n",
    "            net, inp = torch.split(cnet, [hdim, cdim], dim=1)\n",
    "            net = torch.tanh(net)\n",
    "            inp = torch.relu(inp)\n",
    "\n",
    "        coords0, coords1 = self.initialize_flow(image1)\n",
    "\n",
    "        if flow_init is not None:\n",
    "            coords1 = coords1 + flow_init\n",
    "\n",
    "        flow_predictions = []\n",
    "        for itr in range(iters):\n",
    "            coords1 = coords1.detach()\n",
    "            corr = corr_fn(coords1)  # index correlation volume\n",
    "\n",
    "            flow = coords1 - coords0\n",
    "            with autocast(enabled=self.args.mixed_precision):\n",
    "                net, up_mask, delta_flow = self.update_block(net, inp, corr, flow)\n",
    "\n",
    "            # F(t+1) = F(t) + \\Delta(t)\n",
    "            coords1 = coords1 + delta_flow\n",
    "\n",
    "            # upsample predictions\n",
    "            if up_mask is None:\n",
    "                flow_up = upflow8(coords1 - coords0)\n",
    "            else:\n",
    "                flow_up = self.upsample_flow(coords1 - coords0, up_mask)\n",
    "\n",
    "            flow_predictions.append(flow_up)\n",
    "\n",
    "        if test_mode:\n",
    "            return coords1 - coords0, flow_up\n",
    "\n",
    "        return flow_predictions"
   ],
   "id": "aaea96932ffe08a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Flow visualization code used from https://github.com/tomrunia/OpticalFlow_Visualization\n",
    "\n",
    "\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2018 Tom Runia\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to conditions.\n",
    "#\n",
    "# Author: Tom Runia\n",
    "# Date Created: 2018-08-03\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_colorwheel():\n",
    "    \"\"\"\n",
    "    Generates a color wheel for optical flow visualization as presented in:\n",
    "        Baker et al. \"A Database and Evaluation Methodology for Optical Flow\" (ICCV, 2007)\n",
    "        URL: http://vision.middlebury.edu/flow/flowEval-iccv07.pdf\n",
    "\n",
    "    Code follows the original C++ source code of Daniel Scharstein.\n",
    "    Code follows the the Matlab source code of Deqing Sun.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Color wheel\n",
    "    \"\"\"\n",
    "\n",
    "    RY = 15\n",
    "    YG = 6\n",
    "    GC = 4\n",
    "    CB = 11\n",
    "    BM = 13\n",
    "    MR = 6\n",
    "\n",
    "    ncols = RY + YG + GC + CB + BM + MR\n",
    "    colorwheel = np.zeros((ncols, 3))\n",
    "    col = 0\n",
    "\n",
    "    # RY\n",
    "    colorwheel[0:RY, 0] = 255\n",
    "    colorwheel[0:RY, 1] = np.floor(255 * np.arange(0, RY) / RY)\n",
    "    col = col + RY\n",
    "    # YG\n",
    "    colorwheel[col:col + YG, 0] = 255 - np.floor(255 * np.arange(0, YG) / YG)\n",
    "    colorwheel[col:col + YG, 1] = 255\n",
    "    col = col + YG\n",
    "    # GC\n",
    "    colorwheel[col:col + GC, 1] = 255\n",
    "    colorwheel[col:col + GC, 2] = np.floor(255 * np.arange(0, GC) / GC)\n",
    "    col = col + GC\n",
    "    # CB\n",
    "    colorwheel[col:col + CB, 1] = 255 - np.floor(255 * np.arange(CB) / CB)\n",
    "    colorwheel[col:col + CB, 2] = 255\n",
    "    col = col + CB\n",
    "    # BM\n",
    "    colorwheel[col:col + BM, 2] = 255\n",
    "    colorwheel[col:col + BM, 0] = np.floor(255 * np.arange(0, BM) / BM)\n",
    "    col = col + BM\n",
    "    # MR\n",
    "    colorwheel[col:col + MR, 2] = 255 - np.floor(255 * np.arange(MR) / MR)\n",
    "    colorwheel[col:col + MR, 0] = 255\n",
    "    return colorwheel\n",
    "\n",
    "\n",
    "def flow_uv_to_colors(u, v, convert_to_bgr=False):\n",
    "    \"\"\"\n",
    "    Applies the flow color wheel to (possibly clipped) flow components u and v.\n",
    "\n",
    "    According to the C++ source code of Daniel Scharstein\n",
    "    According to the Matlab source code of Deqing Sun\n",
    "\n",
    "    Args:\n",
    "        u (np.ndarray): Input horizontal flow of shape [H,W]\n",
    "        v (np.ndarray): Input vertical flow of shape [H,W]\n",
    "        convert_to_bgr (bool, optional): Convert output image to BGR. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Flow visualization image of shape [H,W,3]\n",
    "    \"\"\"\n",
    "    flow_image = np.zeros((u.shape[0], u.shape[1], 3), np.uint8)\n",
    "    colorwheel = make_colorwheel()  # shape [55x3]\n",
    "    ncols = colorwheel.shape[0]\n",
    "    rad = np.sqrt(np.square(u) + np.square(v))\n",
    "    a = np.arctan2(-v, -u) / np.pi\n",
    "    fk = (a + 1) / 2 * (ncols - 1)\n",
    "    k0 = np.floor(fk).astype(np.int32)\n",
    "    k1 = k0 + 1\n",
    "    k1[k1 == ncols] = 0\n",
    "    f = fk - k0\n",
    "    for i in range(colorwheel.shape[1]):\n",
    "        tmp = colorwheel[:, i]\n",
    "        col0 = tmp[k0] / 255.0\n",
    "        col1 = tmp[k1] / 255.0\n",
    "        col = (1 - f) * col0 + f * col1\n",
    "        idx = (rad <= 1)\n",
    "        col[idx] = 1 - rad[idx] * (1 - col[idx])\n",
    "        col[~idx] = col[~idx] * 0.75  # out of range\n",
    "        # Note the 2-i => BGR instead of RGB\n",
    "        ch_idx = 2 - i if convert_to_bgr else i\n",
    "        flow_image[:, :, ch_idx] = np.floor(255 * col)\n",
    "    return flow_image\n",
    "\n",
    "\n",
    "def flow_to_image(flow_uv, clip_flow=None, convert_to_bgr=False):\n",
    "    \"\"\"\n",
    "    Expects a two dimensional flow image of shape.\n",
    "\n",
    "    Args:\n",
    "        flow_uv (np.ndarray): Flow UV image of shape [H,W,2]\n",
    "        clip_flow (float, optional): Clip maximum of flow values. Defaults to None.\n",
    "        convert_to_bgr (bool, optional): Convert output image to BGR. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Flow visualization image of shape [H,W,3]\n",
    "    \"\"\"\n",
    "    assert flow_uv.ndim == 3, 'input flow must have three dimensions'\n",
    "    assert flow_uv.shape[2] == 2, 'input flow must have shape [H,W,2]'\n",
    "    if clip_flow is not None:\n",
    "        flow_uv = np.clip(flow_uv, 0, clip_flow)\n",
    "    u = flow_uv[:, :, 0]\n",
    "    v = flow_uv[:, :, 1]\n",
    "    rad = np.sqrt(np.square(u) + np.square(v))\n",
    "    rad_max = np.max(rad)\n",
    "    epsilon = 1e-5\n",
    "    u = u / (rad_max + epsilon)\n",
    "    v = v / (rad_max + epsilon)\n",
    "    return flow_uv_to_colors(u, v, convert_to_bgr)"
   ],
   "id": "1a65d546a17f0644",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Optical flow",
   "id": "2eaee4e45b81985b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def viz(img, flo):\n",
    "    img = img[0].permute(1, 2, 0).cpu().numpy()\n",
    "    flo = flo[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # map flow to rgb image\n",
    "    flo = flow_to_image(flo)\n",
    "    img_flo = np.concatenate([img, flo], axis=0)\n",
    "\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.imshow(img_flo / 255.0)\n",
    "    # plt.show()\n",
    "\n",
    "    cv2.imshow('image', img_flo[:, :, [2, 1, 0]] / 255.0)\n",
    "    cv2.waitKey()"
   ],
   "id": "52799836b1a976ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.utils as utils\n",
    "\n",
    "device = utils.get_torch_device();"
   ],
   "id": "37b88245118244c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_path = '../../data/RAFT/raft-things.pth'",
   "id": "fb4abfa7311f7100",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', help=\"restore checkpoint\")\n",
    "parser.add_argument('--path', help=\"dataset for evaluation\")\n",
    "parser.add_argument('--small', action='store_true', help='use small model')\n",
    "parser.add_argument('--mixed_precision', action='store_true', help='use mixed precision')\n",
    "parser.add_argument('--alternate_corr', action='store_true', help='use efficent correlation implementation')\n",
    "\n",
    "args = parser.parse_args([])"
   ],
   "id": "d34a1466d442f4ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = torch.nn.DataParallel(RAFT(args))\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "model = model.module\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    padder = InputPadder(frame1.shape)\n",
    "    image1 = torch.tensor(frame1, device=device, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
    "    image2 = torch.tensor(frame2, device=device, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "    flow_low, flow_up = model(image1, image2, iters=20, test_mode=True)"
   ],
   "id": "7e31007f174f9aa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# display the result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "flow_up = flow_up[0].permute(1, 2, 0).cpu().numpy()\n",
    "flow_up = cv2.resize(flow_up, (frame1.shape[1], frame1.shape[0]))\n",
    "\n",
    "plt.imshow(flow_to_image(flow_up))\n",
    "plt.show()"
   ],
   "id": "43a39869a5167e12",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
