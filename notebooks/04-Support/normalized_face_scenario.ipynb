{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Normalized Faces Scenario\n",
    "\n",
    "This notebook creates a new scenario where the face of the person is the only thing visible in the video."
   ],
   "id": "6c15c98097025cbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# Get the UV map from the internet\n",
    "url = \"https://raw.githubusercontent.com/apple2373/mediapipe-facemesh/main/data/uv_map.json\"\n",
    "response = requests.get(url)\n",
    "\n",
    "uv_map_dict = response.json()\n",
    "uv_map = np.array([(uv_map_dict[\"u\"][str(i)], uv_map_dict[\"v\"][str(i)]) for i in range(468)])\n",
    "\n",
    "dim = 256\n",
    "landmarks_uv = np.array([(dim * x, dim * y) for x, y in uv_map])"
   ],
   "id": "226ee26293d541d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "from mediapipe.tasks.python import vision\n",
    "from skimage.transform import PiecewiseAffineTransform, warp\n",
    "from mediapipe.tasks.python.core import base_options as base_options_module\n",
    "\n",
    "base_options = base_options_module.BaseOptions(\n",
    "    model_asset_path='../../data/mediapipe/face_landmarker_v2_with_blendshapes.task',\n",
    "    delegate=base_options_module.BaseOptions.Delegate.CPU,\n",
    ")\n",
    "options = vision.FaceLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_face_blendshapes=True,\n",
    "    output_facial_transformation_matrixes=True,\n",
    "    num_faces=1,\n",
    ")\n",
    "\n",
    "\n",
    "def normalize_face(frame, landmarks) -> np.ndarray:\n",
    "    # https://scikit-image.org/docs/dev/auto_examples/transform/plot_piecewise_affine.html\n",
    "    tform = PiecewiseAffineTransform()\n",
    "    tform.estimate(landmarks_uv, landmarks)\n",
    "    texture = warp(frame, tform, output_shape=(dim, dim))\n",
    "    texture = (255 * texture).astype(np.uint8)\n",
    "\n",
    "    return texture\n",
    "\n",
    "\n",
    "def normalize_frames(frames: np.ndarray) -> np.ndarray:\n",
    "    detector = vision.FaceLandmarker.create_from_options(options)\n",
    "\n",
    "    height, width = frames[0].shape[0:2]\n",
    "\n",
    "    processed_frames = []\n",
    "\n",
    "    for idx, frame in tqdm(enumerate(frames), total=len(frames)):\n",
    "        image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "        mesh_results = detector.detect(image)\n",
    "        face_landmarks = mesh_results.face_landmarks[0]\n",
    "\n",
    "        # after 468 is iris or something else\n",
    "        face_landmarks = np.array([\n",
    "            (width * point.x, height * point.y) for point in face_landmarks[:468]\n",
    "        ])\n",
    "\n",
    "        normalize_frame = normalize_face(frame, face_landmarks)\n",
    "        processed_frames.append(normalize_frame)\n",
    "\n",
    "    return np.array(processed_frames)"
   ],
   "id": "2638bc4d470f5839",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def store_video_with_mask(frames: np.ndarray, filename: str, fps: int):\n",
    "    shape = frames[0].shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'FFV1')\n",
    "    out = cv2.VideoWriter(filename, fourcc, fps, (shape[1], shape[0]))\n",
    "\n",
    "    for frame in frames:\n",
    "        # Apply the mask to the frame\n",
    "        frame = np.uint8(frame)\n",
    "\n",
    "        # Convert the frame to BGR\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Write the frame\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()"
   ],
   "id": "80763087b93e94b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import subprocess\n",
    "from respiration.dataset import VitalCamSet\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "dataset = VitalCamSet()\n",
    "scenarios = dataset.get_scenarios(['101_natural_lighting'])\n",
    "\n",
    "for (subject, setting) in scenarios:\n",
    "    print(f\"subject: {subject}\")\n",
    "\n",
    "    destination = os.path.join(dataset.data_path, subject, '303_normalized_face')\n",
    "\n",
    "    # Copy the unisens data to the new scenario\n",
    "    source = os.path.join(dataset.data_path, subject, setting, 'synced_Logitech HD Pro Webcam C920')\n",
    "    os.makedirs(destination, exist_ok=True)\n",
    "    subprocess.run([\"cp\", \"-r\", source, destination])\n",
    "\n",
    "    frames, meta = dataset.get_video_rgb(subject, setting)\n",
    "    frames = normalize_frames(frames)\n",
    "\n",
    "    video_path = os.path.join(destination, 'Logitech HD Pro Webcam C920.avi')\n",
    "    store_video_with_mask(frames, video_path, meta.fps)"
   ],
   "id": "11d5d20a08a6a5be",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
