{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Optical Flow\n",
    "\n",
    "This notebook demonstrates the use of the optical flow method to extract the breathing signal from a video. The optical flow method is based on the movement of feature points in the video frames. The movement of the feature points is tracked over time, and the amplitude of the movement is used to extract the breathing signal."
   ],
   "id": "cf3f66d96ef3077"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.dataset as repository\n",
    "\n",
    "dataset = repository.from_default()\n",
    "\n",
    "subject = 'Proband16'\n",
    "setting = '101_natural_lighting'\n",
    "\n",
    "subject_frames, params = dataset.get_video_gray(subject, setting)"
   ],
   "id": "38877d9593e0b11c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hyperparameters = {\n",
    "    'quality_level': 0.1,\n",
    "    'quality_level_rv': 0.05,\n",
    "    'filter_lowpass': 0.1,\n",
    "    'filter_highpass': 0.6,\n",
    "}"
   ],
   "id": "eb6298885b947e0e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import respiration.roi as roi\n",
    "\n",
    "yolo = roi.YOLO()\n",
    "\n",
    "\n",
    "def find_roi(frame: np.ndarray) -> tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Find the region of interest (ROI) based on the face detection\n",
    "    :param frame: The frame to find the ROI\n",
    "    :return: The region of interest (ROI) coordinates (x, y, w, h)\n",
    "    \"\"\"\n",
    "\n",
    "    persons = yolo.detect_classes(frame, 'person')\n",
    "    return persons[0] if len(persons) > 0 else None\n",
    "\n",
    "    # faces = utils.detect_faces(frame)\n",
    "    # \n",
    "    # if len(faces) == 0:\n",
    "    #     raise ValueError('No face detected in the first frame')\n",
    "    # elif len(faces) > 1:\n",
    "    #     raise ValueError('Multiple faces detected in the first frame')\n",
    "    # \n",
    "    # return utils.roi_from_face(faces[0], scale_w=0.2, scale_h=0.2)"
   ],
   "id": "434449646062376d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "frame1 = subject_frames[0]\n",
    "subject_roi = find_roi(frame1)"
   ],
   "id": "57541859a60a4401",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from respiration.extractor import optical_flow\n",
    "\n",
    "# Get different feature points for the first frame\n",
    "default_points = optical_flow.select_feature_points(frame1)\n",
    "special_points = optical_flow.select_feature_points(frame1, fpn=5)\n",
    "roi_points = optical_flow.select_feature_points(frame1, roi=subject_roi)\n",
    "special_roi = optical_flow.select_feature_points(frame1, roi=subject_roi, fpn=5)\n",
    "\n",
    "# Plot the first frame with the feature points\n",
    "plt.imshow(frame1, cmap='gray')\n",
    "\n",
    "# Draw the region of interest (ROI)\n",
    "roi_x, roi_y, roi_w, roi_h = subject_roi\n",
    "plt.gca().add_patch(plt.Rectangle(\n",
    "    (roi_x, roi_y), roi_w, roi_h,\n",
    "    linewidth=1, edgecolor='r', facecolor='none'))\n",
    "\n",
    "for iny in range(default_points.shape[0]):\n",
    "    plt.scatter(default_points[iny, 0, 0],\n",
    "                default_points[iny, 0, 1],\n",
    "                c='r', s=2.5)\n",
    "\n",
    "for iny in range(special_points.shape[0]):\n",
    "    plt.scatter(special_points[iny, 0, 0],\n",
    "                special_points[iny, 0, 1],\n",
    "                c='b', s=2.5)\n",
    "\n",
    "for iny in range(roi_points.shape[0]):\n",
    "    plt.scatter(roi_points[iny, 0, 0],\n",
    "                roi_points[iny, 0, 1],\n",
    "                c='#FFFF00', s=2.5)\n",
    "\n",
    "for iny in range(special_roi.shape[0]):\n",
    "    plt.scatter(special_roi[iny, 0, 0],\n",
    "                special_roi[iny, 0, 1],\n",
    "                c='#FF00FF', s=2.5)\n",
    "\n",
    "plt.show()"
   ],
   "id": "25fee91e19d2492f",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Track the movement of the feature points\n",
    "feature_point_movements = optical_flow.track_feature_point_movement(subject_frames, special_points)\n",
    "\n",
    "# Extract the amplitudes of the feature points\n",
    "raw_signal = optical_flow.calculate_feature_point_amplitudes(feature_point_movements)"
   ],
   "id": "d4e7ef615d5afdfc",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the raw signal\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(raw_signal)"
   ],
   "id": "c0ab063585eb2ded",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.preprocessing as preprocessing\n",
    "\n",
    "preprocessed_unprocessed = optical_flow.signal_from_amplitudes(\n",
    "    raw_signal,\n",
    "    use_cgof=False,\n",
    ")\n",
    "\n",
    "signal_cgof = optical_flow.signal_from_amplitudes(\n",
    "    raw_signal,\n",
    "    use_cgof=True,\n",
    ")\n",
    "signal_filter = preprocessing.butterworth_filter(\n",
    "    signal_cgof,\n",
    "    params.fps,\n",
    "    lowpass=hyperparameters['filter_lowpass'],\n",
    "    highpass=hyperparameters['filter_highpass'],\n",
    ")\n",
    "signal_normalization = preprocessing.normalize_signal(\n",
    "    signal_filter,\n",
    ")\n",
    "\n",
    "processed_signal = signal_normalization"
   ],
   "id": "7f7b5a3048da95a5",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(15, 10))\n",
    "\n",
    "axs[0].plot(preprocessed_unprocessed)\n",
    "axs[0].set_title('Unprocessed')\n",
    "\n",
    "axs[1].plot(signal_cgof)\n",
    "axs[1].set_title('CGOF')\n",
    "\n",
    "axs[2].plot(signal_filter)\n",
    "axs[2].set_title('Filter')\n",
    "\n",
    "plt.show()"
   ],
   "id": "c0df1e526af5308b",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy import signal\n",
    "\n",
    "gt_signal, gt_sample_rate = dataset.get_ground_truth_rr_signal(subject, setting)\n",
    "\n",
    "gt_signal = preprocessing.butterworth_filter(\n",
    "    gt_signal,\n",
    "    gt_sample_rate,\n",
    "    lowpass=hyperparameters['filter_lowpass'],\n",
    "    highpass=hyperparameters['filter_highpass'],\n",
    ")\n",
    "gt_signal = preprocessing.normalize_signal(gt_signal)"
   ],
   "id": "d1ffdc8ee4031b85",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the processed_signal and the gt_signal in the same plot without subplots\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Processed Signal vs Ground Truth Signal')\n",
    "\n",
    "plt.plot(processed_signal, label='Processed Signal')\n",
    "\n",
    "# Down sample the ground truth signal to the same FPS\n",
    "gt_signal_down = signal.resample(gt_signal, len(processed_signal))\n",
    "plt.plot(gt_signal_down, label='Ground Truth Signal')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "fd12a9046a2dc331",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from respiration.analysis import frequency_extractor\n",
    "\n",
    "# Calculate the frequencies using the different methods for the ground truth signal\n",
    "frequency = frequency_extractor.FrequencyExtractor(\n",
    "    gt_signal,\n",
    "    gt_sample_rate,\n",
    "    lowpass=hyperparameters['filter_lowpass'],\n",
    "    highpass=hyperparameters['filter_highpass'],\n",
    ")\n",
    "\n",
    "gt_frequency = {\n",
    "    'FFT': frequency.frequency_from_fft(),\n",
    "    'PC': frequency.frequency_from_peaks(),\n",
    "    'CP': frequency.frequency_from_crossing_point(),\n",
    "    'NFCP': frequency.frequency_from_nfcp()\n",
    "}\n",
    "gt_frequency"
   ],
   "id": "ec52bcb41ebf0536",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for fpn in [None, 5]:\n",
    "    for use_roi in [False, True]:\n",
    "        if use_roi:\n",
    "            roi = subject_roi\n",
    "        else:\n",
    "            roi = None\n",
    "\n",
    "        breathing_signal = optical_flow.extract_signal(\n",
    "            subject_frames,\n",
    "            fpn=fpn,\n",
    "            quality_level=hyperparameters['quality_level'],\n",
    "            quality_level_rv=hyperparameters['quality_level_rv'],\n",
    "            roi=roi,\n",
    "        )\n",
    "\n",
    "        frequency = frequency_extractor.FrequencyExtractor(\n",
    "            breathing_signal,\n",
    "            params.fps,\n",
    "            lowpass=hyperparameters['filter_lowpass'],\n",
    "            highpass=hyperparameters['filter_highpass'],\n",
    "        )\n",
    "\n",
    "        predictions = {\n",
    "            'FFT': frequency.frequency_from_fft(),\n",
    "            'PC': frequency.frequency_from_peaks(),\n",
    "            'CP': frequency.frequency_from_crossing_point(),\n",
    "            'NFCP': frequency.frequency_from_nfcp(),\n",
    "        }\n",
    "\n",
    "        for method, value in predictions.items():\n",
    "            evaluation_results.append({\n",
    "                'fpn': fpn if fpn is not None else 'None',\n",
    "                'use_roi': use_roi,\n",
    "                'method': method,\n",
    "                'value': value,\n",
    "                'error': abs(value - gt_frequency[method]),\n",
    "            })"
   ],
   "id": "175368ef91733e0e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "evaluation = pd.DataFrame(evaluation_results)\n",
    "evaluation"
   ],
   "id": "ce74cf8b6ec65d85",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axs = plt.subplots(4, 1, figsize=(15, 10))\n",
    "\n",
    "# Add more space between the subplots\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "inx = 0\n",
    "for method in gt_frequency.keys():\n",
    "    axs[inx].set_title(f'{method} Method')\n",
    "\n",
    "    # Plot the values for each method\n",
    "    data = evaluation[evaluation['method'] == method]\n",
    "\n",
    "    title = 'fpn=' + data['fpn'].astype(str) + ' / roi=' + data['use_roi'].astype(str)\n",
    "\n",
    "    # Error in Beats Per Minutes (BPM)\n",
    "    axs[inx].bar(title, data['error'] * 60)\n",
    "\n",
    "    inx += 1"
   ],
   "id": "accad4213dd2bd62",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
