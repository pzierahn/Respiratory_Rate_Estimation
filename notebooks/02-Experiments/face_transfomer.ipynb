{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Extracting Respiration Signals with Face Transformer\n",
    "\n",
    "This notebook show how to extract respiration signals from videos using a transformer model and normalized faces."
   ],
   "id": "344c1b3a21abce83"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import respiration.utils as utils\n",
    "\n",
    "model_ids = [\n",
    "    '20240617_134349',\n",
    "    '20240617_213641',\n",
    "    '20240618_093129',\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_size = 256\n",
    "device = utils.get_torch_device()"
   ],
   "id": "10c9d9e53a3e5a58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def preprocess_frames(frames: torch.Tensor) -> torch.Tensor:\n",
    "    # Normalize the frames\n",
    "    frames = (frames - frames.min()) / (frames.max() - frames.min())\n",
    "\n",
    "    # Create a new tensor in the dimensions (batch, channels, 2, height, width)\n",
    "    return torch.stack([frames[:-1], frames[1:]]).permute(1, 2, 0, 3, 4)\n",
    "\n",
    "\n",
    "def signal_diff(time_series: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the difference between two consecutive values in the time series\n",
    "    \"\"\"\n",
    "\n",
    "    # Shift the signal that no negative values are present\n",
    "    min_value = torch.min(time_series)\n",
    "    if min_value < 0:\n",
    "        time_series = time_series - min_value\n",
    "\n",
    "    # Calculate the difference between the time series\n",
    "    diff = time_series[1:] - time_series[:-1]\n",
    "\n",
    "    return diff"
   ],
   "id": "f8bb3c5e958933ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "import respiration.utils as utils\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class ScenarioLoaderChunks:\n",
    "    \"\"\"\n",
    "    A data loader for the VitalCamSet dataset. This class loads the video frames and the ground truth signal for a\n",
    "    specific scenario. The video frames are loaded in chunks of a specific size. The ground truth signal is down-sampled\n",
    "    to match the video frames' dimensions.\n",
    "    \"\"\"\n",
    "    subject: str\n",
    "    setting: str\n",
    "    frames_per_segment: int\n",
    "\n",
    "    def __init__(self,\n",
    "                 subject: str,\n",
    "                 setting: str,\n",
    "                 frames_per_segment: int = 20):\n",
    "        self.subject = subject\n",
    "        self.setting = setting\n",
    "        self.frames_per_segment = frames_per_segment\n",
    "\n",
    "        self.video_path = dataset.get_video_path(subject, setting)\n",
    "        self.total_frames = utils.get_frame_count(self.video_path)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return math.ceil(self.total_frames / self.frames_per_segment)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_index >= self.__len__():\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            item = self.__getitem__(self.current_index)\n",
    "            self.current_index += 1\n",
    "            return item\n",
    "\n",
    "    def __getitem__(self, index) -> (torch.Tensor, torch.Tensor):\n",
    "        \"\"\"\n",
    "        Return the frames and the ground truth signal for the given index\n",
    "        :param index: The index of the chunk\n",
    "        :return: The frames and the ground truth signal\n",
    "        \"\"\"\n",
    "\n",
    "        if index >= self.__len__():\n",
    "            raise IndexError(\"Index out of range\")\n",
    "\n",
    "        start = index * self.frames_per_segment\n",
    "        end = start + self.frames_per_segment\n",
    "        size = min(self.frames_per_segment, self.total_frames - start)\n",
    "\n",
    "        # Load the video frames\n",
    "        frames, meta = utils.read_video_rgb(self.video_path, size, start)\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(mode='RGB'),\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        frames = torch.stack([preprocess(frame) for frame in frames], dim=0)\n",
    "        frames = frames.to(device)\n",
    "\n",
    "        # Get the ground truth signal for the scenario\n",
    "        gt_waveform = dataset.get_breathing_signal(self.subject, self.setting)\n",
    "        gt_waveform = torch.tensor(gt_waveform.copy(), dtype=torch.float32, device=device)\n",
    "        # Normalize the signal between 0 and 1\n",
    "        gt_waveform = (gt_waveform - gt_waveform.min()) / (gt_waveform.max() - gt_waveform.min())\n",
    "        gt_waveform = gt_waveform[start:end]\n",
    "\n",
    "        return frames, gt_waveform"
   ],
   "id": "9c5b09d69b97ac9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from vit_pytorch.simple_vit_3d import SimpleViT\n",
    "\n",
    "\n",
    "def load_model(model_id: str) -> (SimpleViT, dict):\n",
    "    model_dir = utils.dir_path('models', 'transformer', model_id)\n",
    "    manifest_path = utils.join_paths(model_dir, 'manifest.json')\n",
    "    manifest = utils.read_json(manifest_path)\n",
    "\n",
    "    model = SimpleViT(\n",
    "        image_size=image_size,\n",
    "        frames=manifest['frame_patch_size'],\n",
    "        image_patch_size=manifest['image_patch_size'],\n",
    "        frame_patch_size=manifest['frame_patch_size'],\n",
    "        num_classes=1,\n",
    "        dim=manifest['embedding_dim'],\n",
    "        heads=manifest['heads'],\n",
    "        mlp_dim=manifest['mlp_dim'],\n",
    "        depth=manifest['depth'],\n",
    "    ).to(device)\n",
    "\n",
    "    # Load the best model from the training process\n",
    "    model_path = utils.join_paths(model_dir, manifest['trained_models'][-1]['model'])\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model, manifest"
   ],
   "id": "b432d6405d662fba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from respiration.dataset import VitalCamSet\n",
    "\n",
    "dataset = VitalCamSet()"
   ],
   "id": "3657a6b844512f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for model_id in tqdm(model_ids):\n",
    "    model, manifest = load_model(model_id)\n",
    "    scenarios = manifest['testing_scenarios']\n",
    "\n",
    "    for inx, (subject, setting) in enumerate(scenarios):\n",
    "        loader = ScenarioLoaderChunks(subject, setting)\n",
    "\n",
    "        prediction = []\n",
    "\n",
    "        for (frames, gt_classes) in loader:\n",
    "            frames = preprocess_frames(frames)\n",
    "            # Disable gradient computation and reduce memory consumption.\n",
    "            with torch.no_grad():\n",
    "                outputs = model(frames).squeeze()\n",
    "            prediction.extend(outputs.tolist())\n",
    "\n",
    "        predictions.append({\n",
    "            'subject': subject,\n",
    "            'setting': setting,\n",
    "            'model': model_id,\n",
    "            'signal': prediction,\n",
    "        })"
   ],
   "id": "9cd0754594965f72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(predictions)\n",
    "\n",
    "output_dir = utils.dir_path('outputs', 'signals', mkdir=True)\n",
    "\n",
    "# Save the evaluation dataframe\n",
    "csv_path = utils.join_paths(output_dir, 'transformer_predictions.csv')\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "df.head()"
   ],
   "id": "7e3d941678abefce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import respiration.analysis as analysis\n",
    "\n",
    "prediction = predictions[3]\n",
    "subject = prediction['subject']\n",
    "setting = prediction['setting']\n",
    "\n",
    "gt_signal = dataset.get_breathing_signal(subject, setting)\n",
    "prediction_signal = np.array(prediction['signal'])\n",
    "\n",
    "gt_signal = torch.tensor(gt_signal, dtype=torch.float32)\n",
    "gt_signal = signal_diff(gt_signal)\n",
    "gt_signal = np.array(gt_signal)\n",
    "\n",
    "compare = analysis.SignalComparator(\n",
    "    prediction_signal,\n",
    "    gt_signal[:len(prediction_signal)],\n",
    "    30,\n",
    "    detrend_tarvainen=False,\n",
    "    filter_signal=True,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(compare.ground_truth, label='Ground Truth')\n",
    "plt.plot(compare.prediction, label='Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "fa1d21ea88a35ab7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "compare.errors()",
   "id": "d6d6f6977763761b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
