{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transformer",
   "id": "156541bc5c985cd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "import respiration.utils as utils\n",
    "\n",
    "tuned_models = {\n",
    "    '20240512_084854',\n",
    "    '20240512_091356',\n",
    "}\n",
    "\n",
    "# Map model names to their paths\n",
    "models = {}\n",
    "\n",
    "manifests = []\n",
    "\n",
    "for model_id in tuned_models:\n",
    "    model_dir = utils.dir_path('models', 'transformer', model_id)\n",
    "\n",
    "    manifest_path = utils.dir_path(model_dir, 'manifest.json')\n",
    "    manifest = utils.read_json(manifest_path)\n",
    "    best_model = manifest['trained_models'][-1]\n",
    "\n",
    "    model_path = utils.join_paths(model_dir, best_model['model'])\n",
    "    models[model_id] = model_path\n",
    "    manifests.append(manifest)\n",
    "\n",
    "utils.pretty_print(models)"
   ],
   "id": "41913a5f51b91d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from respiration.dataset import VitalCamSet\n",
    "\n",
    "dataset = VitalCamSet()\n",
    "scenarios = dataset.get_scenarios(['101_natural_lighting'])\n",
    "\n",
    "device = utils.get_torch_device()\n",
    "image_size = 256"
   ],
   "id": "3eabcd4e01b673c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def temporal_shifting_frames(frames: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the temporal shifting of the frames. This is done by calculating the difference between the frames and\n",
    "    normalizing the result.\n",
    "    \"\"\"\n",
    "    diff_frames = frames[1:] - frames[:-1]\n",
    "    sum_frames = frames[1:] + frames[:-1]\n",
    "    inputs = diff_frames / (sum_frames + 1e-7)\n",
    "    inputs = (inputs - torch.mean(inputs)) / torch.std(inputs)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def temporal_shifting_signal(time_series: torch.Tensor) -> torch.Tensor:\n",
    "    # Shift the signal that no negative values are present\n",
    "    time_series = time_series - torch.min(time_series)\n",
    "\n",
    "    # Calculate the difference between the time series\n",
    "    diff = time_series[1:] - time_series[:-1]\n",
    "\n",
    "    # Classify the differences into three classes: positive, zero, negative\n",
    "    classes = torch.zeros((diff.shape[0], 3), dtype=torch.float32, device=device)\n",
    "    classes[diff > 0, 0] = 1.0\n",
    "    classes[diff == 0, 1] = 1.0\n",
    "    classes[diff < 0, 2] = 1.0\n",
    "\n",
    "    return classes\n",
    "\n",
    "\n",
    "def signal_from_classes(classes: torch.Tensor) -> np.ndarray:\n",
    "    signal = torch.zeros(classes.shape[0] + 1, dtype=torch.float32, device=device)\n",
    "    signal[1:] = torch.argmax(classes, dim=1) - 1\n",
    "\n",
    "    for idx in range(1, signal.shape[0]):\n",
    "        signal[idx] = signal[idx - 1] + signal[idx]\n",
    "\n",
    "    return signal.cpu().numpy()"
   ],
   "id": "5ff0d6db630bee5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import datetime as dt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from vit_pytorch import SimpleViT\n",
    "from torchvision import transforms\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for (subject, setting) in tqdm(scenarios):\n",
    "    print(f\"Processing {subject} - {setting}\")\n",
    "\n",
    "    video_path = dataset.get_video_path(subject, setting)\n",
    "\n",
    "    frames, _ = utils.read_video_rgb(video_path)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(mode='RGB'),\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    frames = torch.stack([preprocess(frame) for frame in frames], dim=0)\n",
    "    frames = frames.to(device)\n",
    "    frames = temporal_shifting_frames(frames)\n",
    "\n",
    "    for (model_id, model_path) in models.items():\n",
    "        print(f\"--> Using {model_id} model\")\n",
    "        # Wrap modul in nn.DataParallel to fix the model loading issue\n",
    "        model = SimpleViT(\n",
    "            image_size=image_size,\n",
    "            patch_size=32,\n",
    "            num_classes=3,\n",
    "            dim=1024,\n",
    "            depth=6,\n",
    "            heads=16,\n",
    "            mlp_dim=2048\n",
    "        ).to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        start = dt.datetime.now()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(frames)\n",
    "\n",
    "        signal = signal_from_classes(prediction) * -1\n",
    "\n",
    "        predictions.append({\n",
    "            'model': model_id,\n",
    "            'subject': subject,\n",
    "            'setting': setting,\n",
    "            'duration': dt.datetime.now() - start,\n",
    "            'signal': signal.tolist(),\n",
    "        })\n",
    "\n",
    "    del frames"
   ],
   "id": "44cbf5bc27ff119a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the predictions\n",
    "import pandas as pd\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "signals = utils.dir_path('outputs', 'signals')\n",
    "signals_path = utils.join_paths(signals, 'transformer_classifier_predictions.csv')\n",
    "predictions_df.to_csv(signals_path, index=False)"
   ],
   "id": "dfcfd7f0c0ac6273",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import respiration.analysis as analysis\n",
    "\n",
    "subject = 'Proband16'\n",
    "setting = '101_natural_lighting'\n",
    "\n",
    "gt = dataset.get_breathing_signal(subject, setting)\n",
    "gt = torch.tensor(gt, dtype=torch.float32, device=device)\n",
    "xxx = temporal_shifting_signal(gt)\n",
    "yyy = signal_from_classes(xxx) * -1\n",
    "\n",
    "compare = analysis.SignalComparator(gt.cpu().numpy(), yyy, 30)\n",
    "compare.errors()"
   ],
   "id": "60565648f0d4cce8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "compare.signal_distances()",
   "id": "26592327097daacb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(20, 6))\n",
    "\n",
    "# Add some space between the plots\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "axs[0].plot(compare.ground_truth, label='Ground Truth')\n",
    "axs[0].set_title('Ground Truth vs Predicted Signal')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(compare.prediction, label='Predicted')\n",
    "axs[1].set_title('Ground Truth Signal Classes')\n",
    "\n",
    "plt.show()"
   ],
   "id": "3c0a8d7650ca8153",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prediction_subject = predictions_df[(predictions_df['subject'] == subject) &\n",
    "                                    (predictions_df['setting'] == setting)]\n",
    "\n",
    "signal = np.array(prediction_subject['signal'].values[0])\n",
    "compare = analysis.SignalComparator(signal, gt.cpu().numpy(), 30)\n",
    "compare.errors()"
   ],
   "id": "a8ae956c0667b16f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(20, 6))\n",
    "\n",
    "axs[0].plot(compare.ground_truth, label='Ground Truth')\n",
    "axs[0].set_title('Ground Truth vs Predicted Signal')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(compare.prediction, label='Predicted')\n",
    "axs[1].set_title('Ground Truth Signal Classes')\n",
    "\n",
    "plt.show()"
   ],
   "id": "75eb1dd48fcc3513",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
