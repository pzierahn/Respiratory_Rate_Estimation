{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transformer",
   "id": "156541bc5c985cd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.utils as utils\n",
    "\n",
    "tuned_models = {\n",
    "    '20240511_190518',\n",
    "    '20240511_194544',\n",
    "}\n",
    "\n",
    "# Map model names to their paths\n",
    "models = {}\n",
    "\n",
    "manifests = []\n",
    "\n",
    "for model_id in tuned_models:\n",
    "    model_dir = utils.dir_path('models', 'transformer', model_id)\n",
    "\n",
    "    manifest_path = utils.dir_path(model_dir, 'manifest.json')\n",
    "    manifest = utils.read_json(manifest_path)\n",
    "    best_model = manifest['trained_models'][-1]\n",
    "\n",
    "    model_path = utils.join_paths(model_dir, best_model['model'])\n",
    "    models[model_id] = model_path\n",
    "    manifests.append(manifest)\n",
    "\n",
    "utils.pretty_print(models)"
   ],
   "id": "41913a5f51b91d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from respiration.dataset import VitalCamSet\n",
    "\n",
    "dataset = VitalCamSet()\n",
    "scenarios = dataset.get_scenarios(['101_natural_lighting'])\n",
    "\n",
    "device = utils.get_torch_device()\n",
    "image_size = 256"
   ],
   "id": "3eabcd4e01b673c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def temporal_shifting(frames: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the temporal shifting of the frames. This is done by calculating the difference between the frames and\n",
    "    normalizing the result.\n",
    "    \"\"\"\n",
    "    diff_frames = frames[1:] - frames[:-1]\n",
    "    sum_frames = frames[1:] + frames[:-1]\n",
    "    inputs = diff_frames / (sum_frames + 1e-7)\n",
    "    inputs = (inputs - torch.mean(inputs)) / torch.std(inputs)\n",
    "    return inputs"
   ],
   "id": "5ff0d6db630bee5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from vit_pytorch import SimpleViT\n",
    "from torchvision import transforms\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for (subject, setting) in tqdm(scenarios):\n",
    "    print(f\"Processing {subject} - {setting}\")\n",
    "\n",
    "    video_path = dataset.get_video_path(subject, setting)\n",
    "\n",
    "    frames, _ = utils.read_video_rgb(video_path)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(mode='RGB'),\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    frames = torch.stack([preprocess(frame) for frame in frames], dim=0)\n",
    "    frames = frames.to(device)\n",
    "    frames = temporal_shifting(frames)\n",
    "\n",
    "    for (model_id, model_path) in models.items():\n",
    "        print(f\"--> Using {model_id} model\")\n",
    "        # Wrap modul in nn.DataParallel to fix the model loading issue\n",
    "        model = SimpleViT(\n",
    "            image_size=image_size,\n",
    "            patch_size=32,\n",
    "            num_classes=1,\n",
    "            dim=1024,\n",
    "            depth=6,\n",
    "            heads=16,\n",
    "            mlp_dim=2048\n",
    "        ).to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        start = dt.datetime.now()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(frames).cpu().detach().numpy().squeeze()\n",
    "\n",
    "        predictions.append({\n",
    "            'model': model_id,\n",
    "            'subject': subject,\n",
    "            'setting': setting,\n",
    "            'duration': dt.datetime.now() - start,\n",
    "            'signal': prediction.tolist(),\n",
    "        })\n",
    "\n",
    "    del frames\n",
    "\n",
    "predictions = pd.DataFrame(predictions)\n",
    "\n",
    "# Store the predictions to csv\n",
    "signals_dir = utils.dir_path('outputs', 'signals', mkdir=True)\n",
    "signals_path = utils.join_paths(signals_dir, 'transformer_predictions.csv')\n",
    "\n",
    "predictions.to_csv(signals_path, index=False)"
   ],
   "id": "44cbf5bc27ff119a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "predictions.head()",
   "id": "ab2b7b98a1d93f49",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
