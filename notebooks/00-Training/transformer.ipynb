{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformer Classifier\n",
    "\n",
    "This notebook trains a Transformer based classifier to predict inhaling and exhaling from video frames."
   ],
   "id": "67dc06081e64a18c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.utils as utils\n",
    "\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "\n",
    "# The timestamp is the unique identifier for this training run\n",
    "zone = timezone('Europe/Berlin')\n",
    "model_id = datetime.now().astimezone(zone).strftime('%Y%m%d_%H%M%S')\n",
    "device = utils.get_torch_device()\n",
    "\n",
    "# The manifest will store all the metadata for this training run\n",
    "manifest = {\n",
    "    'id': model_id,\n",
    "    'device': str(device),\n",
    "    'timestamp_start': datetime.now().astimezone().isoformat(),\n",
    "    'dataset': 'VitalCamSet',\n",
    "}\n",
    "model_id"
   ],
   "id": "be65cae3ecad860b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "device",
   "id": "4df3cafeef133826",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define training and testing scenarios",
   "id": "a34c6df561579356"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from respiration.dataset import VitalCamSet\n",
    "\n",
    "dataset = VitalCamSet()\n",
    "scenarios_all = dataset.get_scenarios(['101_natural_lighting'])\n",
    "\n",
    "split_ratio = 0.8\n",
    "manifest['split_ratio'] = split_ratio\n",
    "\n",
    "training = scenarios_all[:int(len(scenarios_all) * split_ratio)]\n",
    "manifest['training_scenarios'] = training\n",
    "\n",
    "testing = scenarios_all[int(len(scenarios_all) * split_ratio):]\n",
    "manifest['testing_scenarios'] = testing"
   ],
   "id": "a7a49d6cd0a17760",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_frames = 300\n",
    "manifest['num_frames'] = num_frames\n",
    "\n",
    "frame_patch_size = 2\n",
    "manifest['frame_patch_size'] = frame_patch_size"
   ],
   "id": "92b4df33dcf3161",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test Temporal Shift Module",
   "id": "c2084c18d2e159a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TSMPPG(nn.Module):\n",
    "    def __init__(self, n_segment=10, fold_div=3):\n",
    "        super(TSMPPG, self).__init__()\n",
    "        self.n_segment = n_segment\n",
    "        self.fold_div = fold_div\n",
    "\n",
    "    def forward(self, x):\n",
    "        nt, c, h, w = x.size()\n",
    "        n_batch = nt // self.n_segment  # 30\n",
    "        print(f'nt={nt} c={c} h={h} w={w} n_batch={n_batch}')\n",
    "\n",
    "        x = x.view(n_batch, self.n_segment, c, h, w)\n",
    "        print(f'x.shape={x.shape}')\n",
    "\n",
    "        fold = c // self.fold_div\n",
    "        print(f'fold={fold}')\n",
    "\n",
    "        out = torch.zeros_like(x)\n",
    "        out[:, :-1, :fold] = x[:, 1:, :fold]  # shift left\n",
    "        out[:, 1:, fold: 2 * fold] = x[:, :-1, fold: 2 * fold]  # shift right\n",
    "        out[:, :, 2 * fold:] = x[:, :, 2 * fold:]  # not shift\n",
    "\n",
    "        return out.view(nt, c, h, w)"
   ],
   "id": "80ce8549fa3d0c39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TSMOriginal(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Shift Module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_frame, fold_div=3):\n",
    "        super(TSMOriginal, self).__init__()\n",
    "        self.n_frame = n_frame\n",
    "        self.fold_div = fold_div\n",
    "\n",
    "    def forward(self, x):\n",
    "        nt, c, h, w = x.size()\n",
    "        print(f'nt={nt} c={c} h={h} w={w}')\n",
    "\n",
    "        x = x.view(nt // self.n_frame, self.n_frame, c, h, w)\n",
    "        print(f'x.shape={x.shape}')\n",
    "\n",
    "        fold = c // self.fold_div\n",
    "        print(f'fold={fold}')\n",
    "\n",
    "        last_fold = c - (self.fold_div - 1) * fold\n",
    "        print(f'last_fold={last_fold}')\n",
    "\n",
    "        r_channel, g_channel, b_channel = torch.split(x, [fold, fold, last_fold], dim=2)\n",
    "        print(f'channel.shape={r_channel.shape}')\n",
    "\n",
    "        # Shift left\n",
    "        padding_1 = torch.zeros_like(r_channel[:, -1, :, :, :])\n",
    "        print(f'padding_1.shape={padding_1.shape}')\n",
    "        padding_1 = padding_1.unsqueeze(1)\n",
    "        print(f'padding_1.shape={padding_1.shape}')\n",
    "\n",
    "        _, out1 = torch.split(r_channel, [1, self.n_frame - 1], dim=1)\n",
    "        print(f'out1.shape={out1.shape}')\n",
    "\n",
    "        out1 = torch.cat([out1, padding_1], dim=1)\n",
    "        print(f'out1.shape={out1.shape}')\n",
    "\n",
    "        # Shift right\n",
    "        padding_2 = torch.zeros_like(g_channel[:, 0, :, :, :])\n",
    "        padding_2 = padding_2.unsqueeze(1)\n",
    "        out2, _ = torch.split(g_channel, [self.n_frame - 1, 1], dim=1)\n",
    "        out2 = torch.cat([padding_2, out2], dim=1)\n",
    "\n",
    "        out = torch.cat([out1, out2, b_channel], dim=2)\n",
    "        out = out.view(nt, c, h, w)\n",
    "\n",
    "        return out"
   ],
   "id": "6944097aa48226d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "subject = 'Proband16'\n",
    "setting = '101_natural_lighting'\n",
    "\n",
    "frames, meta = dataset.get_video_rgb(subject, setting, 300, start_position=200)\n",
    "frames = torch.tensor(frames, dtype=torch.float32, device=device)\n",
    "frames = frames.permute(0, 3, 1, 2)\n",
    "\n",
    "tsm = TSMPPG(300).to(device)\n",
    "tsm_frames = tsm(frames)"
   ],
   "id": "78d008a0881d26e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_frame = tsm_frames[10].permute(1, 2, 0).cpu().detach().numpy()\n",
    "\n",
    "# Create a gird and display all three channels\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 20))\n",
    "\n",
    "for idx in range(3):\n",
    "    ax[idx].imshow(test_frame[:, :, idx], cmap='gray')\n",
    "    ax[idx].axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "id": "8ef17b22a163cd47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Show the test frame\n",
    "plt.imshow(test_frame / 255.0)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "a3217456a99c7469",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Show the diff frame\n",
    "diff = frames[0] - frames[10]\n",
    "diff = diff.permute(1, 2, 0).cpu().detach().numpy()\n",
    "\n",
    "# Normalize the diff frame\n",
    "diff = (diff - diff.min()) / (diff.max() - diff.min())\n",
    "\n",
    "plt.imshow(diff)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "991284ebc0e690fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define the model",
   "id": "6cf51af626d8711"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def temporal_shifting_frames(frames: torch.Tensor) -> torch.Tensor:\n",
    "    # Use unfold to create sliding windows\n",
    "    diff_video = frames[1:] - frames[:-1]\n",
    "\n",
    "    # Normalize the diff video between 0 and 1\n",
    "    diff_video = (diff_video - diff_video.min()) / (diff_video.max() - diff_video.min())\n",
    "\n",
    "    return diff_video\n",
    "\n",
    "\n",
    "def temporal_shifting_signal(time_series: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a binary signal from the time series. The signal is 1 if the value is greater than the previous value, and 0 otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Shift the signal that no negative values are present\n",
    "    min_value = torch.min(time_series)\n",
    "    if min_value < 0:\n",
    "        time_series = time_series - min_value\n",
    "\n",
    "    # Calculate the difference between the time series\n",
    "    diff = time_series[1:] - time_series[:-1]\n",
    "\n",
    "    # Make all >0 values 1 and all <0 values 0\n",
    "    diff = torch.where(diff > 0, torch.tensor(1.0, device=device), torch.tensor(0.0, device=device))\n",
    "\n",
    "    # Make to int\n",
    "    diff = diff.to(torch.long)\n",
    "\n",
    "    return diff"
   ],
   "id": "d84718446ec15852",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_size = 224\n",
    "manifest['image_size'] = image_size\n",
    "\n",
    "# Keep every 10th data point\n",
    "downsample_factor = 10\n",
    "manifest['downsample_factor'] = downsample_factor"
   ],
   "id": "6045eb1b0069b2bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import respiration.utils as utils\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class ScenarioLoader:\n",
    "    \"\"\"\n",
    "    A data loader for the VitalCamSet dataset. This class loads the video frames and the ground truth signal for a\n",
    "    specific scenario. The video frames are loaded in chunks of a specific size. The ground truth signal is down-sampled\n",
    "    to match the video frames' dimensions.\n",
    "    \"\"\"\n",
    "    subject: str\n",
    "    setting: str\n",
    "\n",
    "    def __init__(self,\n",
    "                 subject: str,\n",
    "                 setting: str):\n",
    "        self.subject = subject\n",
    "        self.setting = setting\n",
    "\n",
    "        self.video_path = dataset.get_video_path(subject, setting)\n",
    "        self.total_frames = utils.get_frame_count(self.video_path)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return downsample_factor\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_index >= self.__len__():\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            item = self.__getitem__(self.current_index)\n",
    "            self.current_index += 1\n",
    "            return item\n",
    "\n",
    "    def __getitem__(self, index) -> (torch.Tensor, torch.Tensor):\n",
    "        \"\"\"\n",
    "        Return the frames and the ground truth signal for the given index\n",
    "        :param index: The index of the chunk\n",
    "        :return: The frames and the ground truth signal\n",
    "        \"\"\"\n",
    "\n",
    "        if index >= self.__len__():\n",
    "            raise IndexError(\"Index out of range\")\n",
    "\n",
    "        # Load the video frames\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "\n",
    "        frames = []\n",
    "        for position in range(0, self.total_frames, downsample_factor):\n",
    "            # Seek to the down-sampled position in the video\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, position + index)\n",
    "\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        frames = np.array([cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) for frame in frames])\n",
    "\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(mode='RGB'),\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        frames = torch.stack([preprocess(frame) for frame in frames], dim=0)\n",
    "        frames = frames.to(device)\n",
    "\n",
    "        # Get the ground truth signal for the scenario\n",
    "        gt_waveform = dataset.get_breathing_signal(self.subject, self.setting)\n",
    "        gt_waveform = torch.tensor(gt_waveform, dtype=torch.float32, device=device)\n",
    "        gt_waveform = torch.nn.functional.normalize(gt_waveform, dim=0)\n",
    "        gt_waveform = gt_waveform[index::downsample_factor]\n",
    "\n",
    "        return frames, gt_waveform"
   ],
   "id": "3ec25f327cb4b1c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def count_classes(xxx):\n",
    "    xxx = torch.argmax(xxx, dim=1)\n",
    "    count = np.unique(xxx.cpu().detach().numpy(), return_counts=True)\n",
    "    return count"
   ],
   "id": "d5d657bc586aa286",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test the ScenarioLoader",
   "id": "157cbbbee85c69ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loader = ScenarioLoader('Proband16', '101_natural_lighting')\n",
    "\n",
    "chunk_frames, chunk_signal = loader[0]\n",
    "chunk_frames.shape, chunk_signal.shape"
   ],
   "id": "39481a906bbf2d91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diff = chunk_frames[0] - chunk_frames[1]\n",
    "diff = diff.permute(1, 2, 0).cpu().detach().numpy()\n",
    "\n",
    "# Normalize the diff frame\n",
    "diff = (diff - diff.min()) / (diff.max() - diff.min())\n",
    "\n",
    "plt.imshow(diff)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "d8dc79443d140ee7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "shifted = temporal_shifting_frames(chunk_frames)\n",
    "test_frame = shifted[10].permute(1, 2, 0).cpu().detach().numpy()\n",
    "\n",
    "# Show the diff frame\n",
    "plt.imshow(test_frame)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "5106cd244351d362",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Training",
   "id": "1cccdc3abd23e9a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = utils.dir_path('outputs', 'logs', model_id, mkdir=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ],
   "id": "8c815980f7431cb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from vit_pytorch import SimpleViT\n",
    "\n",
    "# 2 classes: inhale, exhale\n",
    "num_classes = 2\n",
    "manifest['num_classes'] = num_classes\n",
    "\n",
    "image_patch_size = 16\n",
    "manifest['image_patch_size'] = image_patch_size\n",
    "\n",
    "depth = 6\n",
    "manifest['depth'] = depth\n",
    "\n",
    "heads = 16\n",
    "manifest['heads'] = heads\n",
    "\n",
    "mlp_dim = 2048\n",
    "manifest['mlp_dim'] = mlp_dim\n",
    "\n",
    "embedding_dim = 512\n",
    "manifest['embedding_dim'] = embedding_dim\n",
    "\n",
    "spatial_depth = 6\n",
    "manifest['spatial_depth'] = spatial_depth\n",
    "\n",
    "temporal_depth = 6\n",
    "manifest['temporal_depth'] = temporal_depth\n",
    "\n",
    "model = SimpleViT(\n",
    "    image_size=image_size,\n",
    "    patch_size=image_patch_size,\n",
    "    num_classes=num_classes,\n",
    "    dim=embedding_dim,\n",
    "    heads=heads,\n",
    "    mlp_dim=mlp_dim,\n",
    "    depth=depth,\n",
    ").to(device)\n",
    "manifest['base_model'] = 'SimpleViT'"
   ],
   "id": "bcc71511f5183e9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "epochs = 30\n",
    "manifest['epochs'] = epochs\n",
    "\n",
    "learning_rate = 0.00001\n",
    "manifest['learning_rate'] = learning_rate\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "manifest['loss_fn'] = 'CrossEntropyLoss'\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "manifest['optimizer'] = 'AdamW'"
   ],
   "id": "1f16e5efdceab177",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_one_epoch(epoch_index: int):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Iterate over the training scenarios\n",
    "    for (subject, setting) in training:\n",
    "        loader = ScenarioLoader(subject, setting)\n",
    "\n",
    "        scenario_accuracy = 0.0\n",
    "\n",
    "        # Iterate over the hole scenario video in chunks\n",
    "        for idy, (frames, gt_classes) in enumerate(loader):\n",
    "            frames = temporal_shifting_frames(frames)\n",
    "\n",
    "            gt_classes = temporal_shifting_signal(gt_classes)\n",
    "            # Cut the gt_classes to match the frames\n",
    "            gt_classes = gt_classes[:frames.shape[0]]\n",
    "\n",
    "            # Make predictions for this chunk\n",
    "            outputs = model(frames)\n",
    "            predicted_classes = outputs.argmax(dim=1)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_fn(outputs, gt_classes)\n",
    "\n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Gather data and report\n",
    "            accuracy = (predicted_classes == gt_classes).float().mean()\n",
    "            print(f'  {subject} #{idy:02d} outputs.count={count_classes(outputs)} accuracy={accuracy}')\n",
    "            scenario_accuracy += accuracy\n",
    "\n",
    "        scenario_accuracy /= len(loader)\n",
    "        epoch_loss += scenario_accuracy\n",
    "\n",
    "        print(f'  >> {subject} accuracy={scenario_accuracy}')\n",
    "        writer.add_scalars('Training_Accuracy', {\n",
    "            f'{subject}_{setting}': scenario_accuracy,\n",
    "        }, epoch_index)\n",
    "        writer.flush()\n",
    "\n",
    "    return epoch_loss / len(training)"
   ],
   "id": "c5f12b4ed685e672",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_dir = utils.dir_path('models', 'transformer', model_id, mkdir=True)",
   "id": "bddc7e7fbb29988",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save_manifest(best_accuracy):\n",
    "    manifest['trained_models'] = models\n",
    "    manifest['best_testing_accuracy'] = float(best_accuracy)\n",
    "    manifest['timestamp_finish'] = datetime.now().astimezone().isoformat()\n",
    "    utils.write_json(os.path.join(model_dir, 'manifest.json'), manifest)"
   ],
   "id": "7e66bf39b3816485",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "best_accuracy = 0.0\n",
    "models = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f'Epoch {epoch}:')\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch)\n",
    "\n",
    "    running_accuracy = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for inx, (subject, setting) in enumerate(testing):\n",
    "            loader = ScenarioLoader(subject, setting)\n",
    "            testing_accuracy = 0.0\n",
    "\n",
    "            for (frames, gt_classes) in loader:\n",
    "                frames = temporal_shifting_frames(frames)\n",
    "                gt_classes = temporal_shifting_signal(gt_classes)\n",
    "                gt_classes = gt_classes[:frames.shape[0]]\n",
    "                voutputs = model(frames)\n",
    "                testing_accuracy += (voutputs.argmax(dim=1) == gt_classes).float().mean()\n",
    "\n",
    "            testing_accuracy /= len(loader)\n",
    "            writer.add_scalars('Testing_Loss', {f'{subject}_{setting}': testing_accuracy}, epoch)\n",
    "            print(f'  >> {subject} accuracy={testing_accuracy}')\n",
    "\n",
    "            running_accuracy += testing_accuracy\n",
    "\n",
    "    testing_accuracy = running_accuracy / len(testing)\n",
    "    print(f'LOSS training={avg_loss} testing={testing_accuracy}')\n",
    "    writer.add_scalars('Average_Accuracy', {\n",
    "        'Training': avg_loss,\n",
    "        'Testing': testing_accuracy,\n",
    "    }, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track the best performance, and save the model's state\n",
    "    if testing_accuracy > best_accuracy:\n",
    "        best_accuracy = testing_accuracy\n",
    "        model_name = f'{model_id}_{epoch}.pth'\n",
    "\n",
    "        model_path = os.path.join(model_dir, model_name)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        models.append({\n",
    "            'model': model_name,\n",
    "            'epoch': epoch,\n",
    "            'validation_accuracy': float(testing_accuracy),\n",
    "        })\n",
    "        save_manifest(testing_accuracy)"
   ],
   "id": "63f1645676dd1690",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "save_manifest(best_accuracy)",
   "id": "7c44122f4d4d4657",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
