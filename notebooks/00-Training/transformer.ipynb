{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformer Classifier\n",
    "\n",
    "This notebook trains a Transformer based classifier to predict inhaling and exhaling from video frames."
   ],
   "id": "67dc06081e64a18c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.utils as utils\n",
    "\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "\n",
    "# The timestamp is the unique identifier for this training run\n",
    "zone = timezone('Europe/Berlin')\n",
    "model_id = datetime.now().astimezone(zone).strftime('%Y%m%d_%H%M%S')\n",
    "device = utils.get_torch_device()\n",
    "\n",
    "# The manifest will store all the metadata for this training run\n",
    "manifest = {\n",
    "    'id': model_id,\n",
    "    'device': str(device),\n",
    "    'timestamp_start': datetime.now().astimezone().isoformat(),\n",
    "    'dataset': 'VitalCamSet',\n",
    "}\n",
    "model_id"
   ],
   "id": "be65cae3ecad860b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "device",
   "id": "4df3cafeef133826",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define training and testing scenarios",
   "id": "a34c6df561579356"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from respiration.dataset import VitalCamSet\n",
    "\n",
    "dataset = VitalCamSet()\n",
    "scenarios_all = dataset.get_scenarios(['101_natural_lighting'])\n",
    "\n",
    "split_ratio = 0.8\n",
    "manifest['split_ratio'] = split_ratio\n",
    "\n",
    "training = scenarios_all[:int(len(scenarios_all) * split_ratio)]\n",
    "manifest['training_scenarios'] = training\n",
    "\n",
    "testing = scenarios_all[int(len(scenarios_all) * split_ratio):]\n",
    "manifest['testing_scenarios'] = testing"
   ],
   "id": "a7a49d6cd0a17760",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_frames = 300\n",
    "manifest['num_frames'] = num_frames\n",
    "\n",
    "frame_patch_size = 2\n",
    "manifest['frame_patch_size'] = frame_patch_size"
   ],
   "id": "92b4df33dcf3161",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define temporal shifting",
   "id": "6cf51af626d8711"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def preprocess_frames(frames: torch.Tensor) -> torch.Tensor:\n",
    "    diff_frames = frames[1:] - frames[:-1]\n",
    "    sum_frames = frames[1:] + frames[:-1]\n",
    "\n",
    "    shifted_frames = diff_frames / (sum_frames + 1e-7)\n",
    "    shifted_frames = (shifted_frames - shifted_frames.min()) / (shifted_frames.max() - shifted_frames.min() + 1e-7)\n",
    "    \n",
    "    # Normalize the frames\n",
    "    frames = (frames - frames.min()) / (frames.max() - frames.min())\n",
    "\n",
    "    # Remove the first frame because it has no diff\n",
    "    frames = frames[1:]\n",
    "\n",
    "    # Create a new tensor in the dimensions (batch, channels, 2, height, width)\n",
    "    return torch.stack([frames, shifted_frames]).permute(1, 2, 0, 3, 4)\n",
    "\n",
    "\n",
    "def preprocess_signal(time_series: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a binary signal from the time series. The signal is 1 if the value is greater than the previous value, and 0 otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Shift the signal that no negative values are present\n",
    "    min_value = torch.min(time_series)\n",
    "    if min_value < 0:\n",
    "        time_series = time_series - min_value\n",
    "\n",
    "    # Calculate the difference between the time series\n",
    "    diff = time_series[1:] - time_series[:-1]\n",
    "\n",
    "    # Make all >0 values 1 and all <0 values 0\n",
    "    diff = torch.where(diff > 0, torch.tensor(1.0, device=device), torch.tensor(0.0, device=device))\n",
    "\n",
    "    # Make to int\n",
    "    diff = diff.to(torch.long)\n",
    "\n",
    "    return diff"
   ],
   "id": "d84718446ec15852",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_size = 224\n",
    "manifest['image_size'] = image_size\n",
    "\n",
    "# Keep every 10th data point\n",
    "downsample_factor = 10\n",
    "manifest['downsample_factor'] = downsample_factor"
   ],
   "id": "6045eb1b0069b2bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import respiration.utils as utils\n",
    "\n",
    "\n",
    "class ScenarioLoader:\n",
    "    \"\"\"\n",
    "    A data loader for the VitalCamSet dataset. This class loads the video frames and the ground truth signal for a\n",
    "    specific scenario. The video frames are loaded in chunks of a specific size. The ground truth signal is down-sampled\n",
    "    to match the video frames' dimensions.\n",
    "    \"\"\"\n",
    "    subject: str\n",
    "    setting: str\n",
    "    frames_per_segment: int\n",
    "\n",
    "    def __init__(self,\n",
    "                 subject: str,\n",
    "                 setting: str,\n",
    "                 frames_per_segment: int = num_frames):\n",
    "        self.subject = subject\n",
    "        self.setting = setting\n",
    "        self.frames_per_segment = frames_per_segment\n",
    "\n",
    "        self.video_path = dataset.get_video_path(subject, setting)\n",
    "        self.total_frames = utils.get_frame_count(self.video_path)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return math.ceil(self.total_frames / self.frames_per_segment)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_index >= self.__len__():\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            item = self.__getitem__(self.current_index)\n",
    "            self.current_index += 1\n",
    "            return item\n",
    "\n",
    "    def __getitem__(self, index) -> (torch.Tensor, torch.Tensor):\n",
    "        \"\"\"\n",
    "        Return the frames and the ground truth signal for the given index\n",
    "        :param index: The index of the chunk\n",
    "        :return: The frames and the ground truth signal\n",
    "        \"\"\"\n",
    "\n",
    "        if index >= self.__len__():\n",
    "            raise IndexError(\"Index out of range\")\n",
    "\n",
    "        start = index * self.frames_per_segment\n",
    "        end = start + self.frames_per_segment\n",
    "        size = min(self.frames_per_segment, self.total_frames - start)\n",
    "\n",
    "        # Load the video frames\n",
    "        frames, _ = utils.read_video_rgb(self.video_path, size, start)\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(mode='RGB'),\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        frames = torch.stack([preprocess(frame) for frame in frames], dim=0)\n",
    "        frames = frames.to(device)\n",
    "\n",
    "        # Get the ground truth signal for the scenario\n",
    "        gt_waveform = dataset.get_breathing_signal(self.subject, self.setting)\n",
    "        gt_waveform = torch.tensor(gt_waveform, dtype=torch.float32, device=device)\n",
    "        gt_waveform = torch.nn.functional.normalize(gt_waveform, dim=0)\n",
    "        gt_waveform = gt_waveform[start:end]\n",
    "\n",
    "        return frames, gt_waveform"
   ],
   "id": "3ec25f327cb4b1c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def count_classes(xxx):\n",
    "    xxx = torch.argmax(xxx, dim=1)\n",
    "    count = np.unique(xxx.cpu().detach().numpy(), return_counts=True)\n",
    "    return count"
   ],
   "id": "d5d657bc586aa286",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test the ScenarioLoader",
   "id": "157cbbbee85c69ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loader = ScenarioLoader('Proband16', '101_natural_lighting')\n",
    "\n",
    "chunk_frames, chunk_signal = loader[0]\n",
    "chunk_frames.shape, chunk_signal.shape"
   ],
   "id": "39481a906bbf2d91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Process the frames (batch, channels, frames, height, width)\n",
    "shifted = preprocess_frames(chunk_frames)\n",
    "print(f'shifted={shifted.shape}')\n",
    "\n",
    "frame1, frame2 = shifted[0].permute(1, 0, 2, 3)\n",
    "print(f'test_frame={frame1.shape}')\n",
    "\n",
    "# Show the diff frame\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(frame1.cpu().permute(1, 2, 0))\n",
    "ax[1].imshow(frame2.cpu().permute(1, 2, 0))\n",
    "plt.show()"
   ],
   "id": "5106cd244351d362",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Training",
   "id": "1cccdc3abd23e9a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = utils.dir_path('outputs', 'logs', model_id, mkdir=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ],
   "id": "8c815980f7431cb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from vit_pytorch.simple_vit_3d import SimpleViT\n",
    "\n",
    "# 2 classes: inhale, exhale\n",
    "num_classes = 2\n",
    "manifest['num_classes'] = num_classes\n",
    "\n",
    "image_patch_size = 16\n",
    "manifest['image_patch_size'] = image_patch_size\n",
    "\n",
    "depth = 6\n",
    "manifest['depth'] = depth\n",
    "\n",
    "heads = 16\n",
    "manifest['heads'] = heads\n",
    "\n",
    "mlp_dim = 2048\n",
    "manifest['mlp_dim'] = mlp_dim\n",
    "\n",
    "embedding_dim = 512\n",
    "manifest['embedding_dim'] = embedding_dim\n",
    "\n",
    "spatial_depth = 6\n",
    "manifest['spatial_depth'] = spatial_depth\n",
    "\n",
    "temporal_depth = 6\n",
    "manifest['temporal_depth'] = temporal_depth\n",
    "\n",
    "model = SimpleViT(\n",
    "    image_size=image_size,\n",
    "    frames=2,\n",
    "    image_patch_size=image_patch_size,\n",
    "    frame_patch_size=frame_patch_size,\n",
    "    num_classes=num_classes,\n",
    "    dim=embedding_dim,\n",
    "    heads=heads,\n",
    "    mlp_dim=mlp_dim,\n",
    "    depth=depth,\n",
    ").to(device)\n",
    "manifest['base_model'] = 'simple_vit_3d'"
   ],
   "id": "bcc71511f5183e9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "epochs = 30\n",
    "manifest['epochs'] = epochs\n",
    "\n",
    "learning_rate = 0.00001\n",
    "manifest['learning_rate'] = learning_rate\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "manifest['loss_fn'] = 'CrossEntropyLoss'\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "manifest['optimizer'] = 'AdamW'"
   ],
   "id": "1f16e5efdceab177",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_one_epoch(epoch_index: int):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Iterate over the training scenarios\n",
    "    for (subject, setting) in training:\n",
    "        loader = ScenarioLoader(subject, setting)\n",
    "\n",
    "        scenario_accuracy = 0.0\n",
    "\n",
    "        # Iterate over the hole scenario video in chunks\n",
    "        for idy, (frames, gt_classes) in enumerate(loader):\n",
    "            frames = preprocess_frames(frames)\n",
    "\n",
    "            gt_classes = preprocess_signal(gt_classes)\n",
    "            # Cut the gt_classes to match the frames\n",
    "            gt_classes = gt_classes[:frames.shape[0]]\n",
    "\n",
    "            # Make predictions for this chunk\n",
    "            outputs = model(frames)\n",
    "            predicted_classes = outputs.argmax(dim=1)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_fn(outputs, gt_classes)\n",
    "\n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Gather data and report\n",
    "            accuracy = (predicted_classes == gt_classes).float().mean()\n",
    "            print(f'  {subject} #{idy:02d} outputs.count={count_classes(outputs)} accuracy={accuracy}')\n",
    "            scenario_accuracy += accuracy\n",
    "\n",
    "        scenario_accuracy /= len(loader)\n",
    "        epoch_loss += scenario_accuracy\n",
    "\n",
    "        print(f'  >> {subject} accuracy={scenario_accuracy}')\n",
    "        writer.add_scalars('Training_Accuracy', {\n",
    "            f'{subject}_{setting}': scenario_accuracy,\n",
    "        }, epoch_index)\n",
    "        writer.flush()\n",
    "\n",
    "    return epoch_loss / len(training)"
   ],
   "id": "c5f12b4ed685e672",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_dir = utils.dir_path('models', 'transformer', model_id, mkdir=True)",
   "id": "bddc7e7fbb29988",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save_manifest(best_accuracy):\n",
    "    manifest['trained_models'] = models\n",
    "    manifest['best_testing_accuracy'] = float(best_accuracy)\n",
    "    manifest['timestamp_finish'] = datetime.now().astimezone().isoformat()\n",
    "    utils.write_json(os.path.join(model_dir, 'manifest.json'), manifest)"
   ],
   "id": "7e66bf39b3816485",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "best_accuracy = 0.0\n",
    "models = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f'Epoch {epoch}:')\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch)\n",
    "\n",
    "    running_accuracy = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for inx, (subject, setting) in enumerate(testing):\n",
    "            loader = ScenarioLoader(subject, setting)\n",
    "            testing_accuracy = 0.0\n",
    "\n",
    "            for (frames, gt_classes) in loader:\n",
    "                frames = preprocess_frames(frames)\n",
    "                gt_classes = preprocess_signal(gt_classes)\n",
    "                gt_classes = gt_classes[:frames.shape[0]]\n",
    "                voutputs = model(frames)\n",
    "                testing_accuracy += (voutputs.argmax(dim=1) == gt_classes).float().mean()\n",
    "\n",
    "            testing_accuracy /= len(loader)\n",
    "            writer.add_scalars('Testing_Loss', {f'{subject}_{setting}': testing_accuracy}, epoch)\n",
    "            print(f'  >> {subject} accuracy={testing_accuracy}')\n",
    "\n",
    "            running_accuracy += testing_accuracy\n",
    "\n",
    "    testing_accuracy = running_accuracy / len(testing)\n",
    "    print(f'LOSS training={avg_loss} testing={testing_accuracy}')\n",
    "    writer.add_scalars('Average_Accuracy', {\n",
    "        'Training': avg_loss,\n",
    "        'Testing': testing_accuracy,\n",
    "    }, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track the best performance, and save the model's state\n",
    "    if testing_accuracy > best_accuracy:\n",
    "        best_accuracy = testing_accuracy\n",
    "        model_name = f'{model_id}_{epoch}.pth'\n",
    "\n",
    "        model_path = os.path.join(model_dir, model_name)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        models.append({\n",
    "            'model': model_name,\n",
    "            'epoch': epoch,\n",
    "            'validation_accuracy': float(testing_accuracy),\n",
    "        })\n",
    "        save_manifest(testing_accuracy)"
   ],
   "id": "63f1645676dd1690",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "save_manifest(best_accuracy)",
   "id": "7c44122f4d4d4657",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
