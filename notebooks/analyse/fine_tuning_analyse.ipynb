{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analyze Fine-Tuned EfficientPhys Models",
   "id": "156541bc5c985cd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the Predictions",
   "id": "f6fbc6544ceaedce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import respiration.utils as utils\n",
    "\n",
    "evaluation_dir = os.path.join('..', 'evaluation', 'efficeint_phys_fine_tuned')\n",
    "evaluation_path = os.path.join(evaluation_dir, 'predictions.csv')\n",
    "\n",
    "prediction = pd.read_csv(evaluation_path)\n",
    "prediction['signal'] = prediction['signal'].apply(eval).apply(np.array)"
   ],
   "id": "36f66e8a71704a30",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "prediction.head()",
   "id": "3c049d5de449d5a1",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "prediction['id'].unique()",
   "id": "821c1f6999349b6b",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the model specifications",
   "id": "3b4bdbed6d3dd97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "manifest_path = os.path.join(evaluation_dir, 'manifests.json')\n",
    "manifests = utils.read_json(manifest_path)\n",
    "manifests = {manifest['id']: manifest for manifest in manifests}"
   ],
   "id": "58c648cbfb4dce6a",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Get the Ground Truth RR Signal",
   "id": "e925da72b92c6338"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ground_truth_file = os.path.join(os.getcwd(), '..', 'evaluation', 'ground_truth.csv')\n",
    "ground_truth = pd.read_csv(ground_truth_file)\n",
    "ground_truth['signal'] = ground_truth['signal'].apply(eval).apply(np.array)\n",
    "ground_truth.head()"
   ],
   "id": "aa5deabe3ffec84d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analyze the Performance of the Model",
   "id": "cda20c512d3a9b55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import scipy.signal as signal\n",
    "import respiration.preprocessing as preprocessing\n",
    "\n",
    "\n",
    "def get_ground_truth(subject, setting, sample_rate: int, length: int) -> np.ndarray:\n",
    "    subject_gt = ground_truth[(ground_truth['subject'] == subject) &\n",
    "                              (ground_truth['setting'] == setting)].iloc[0]\n",
    "\n",
    "    gt_signal = subject_gt['signal']\n",
    "\n",
    "    # Normalize the signal\n",
    "    gt_signal = preprocessing.normalize_signal(gt_signal)\n",
    "\n",
    "    # Resample the signal to sample_rate\n",
    "    gt_signal = signal.resample(gt_signal, int(len(gt_signal) * sample_rate / subject_gt['sample_rate']))\n",
    "\n",
    "    # Cut the signal to the same length as the prediction\n",
    "    gt_signal = gt_signal[:length]\n",
    "\n",
    "    return gt_signal"
   ],
   "id": "bac371b34b2ee4c7",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_prediction(subject, setting, model_id) -> tuple[np.ndarray, int]:\n",
    "    subject_pred = prediction[(prediction['id'] == model_id) &\n",
    "                              (prediction['subject'] == subject) &\n",
    "                              (prediction['setting'] == setting)].iloc[0]\n",
    "\n",
    "    sample_rate = subject_pred['sample_rate']\n",
    "\n",
    "    pred_signal = subject_pred['signal']\n",
    "    # pred_signal = preprocessing.standard_processing(pred_signal, sample_rate)\n",
    "\n",
    "    return pred_signal, sample_rate"
   ],
   "id": "7ddd3d92ec299fe6",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import respiration.dataset as repository\n",
    "\n",
    "dataset = repository.from_default()\n",
    "\n",
    "subject = 'Proband25'\n",
    "setting = '101_natural_lighting'\n",
    "model_id = '20240504_163423'\n",
    "\n",
    "pred_signal, sample_rate = get_prediction(subject, setting, model_id)\n",
    "gt_signal = get_ground_truth(subject, setting, sample_rate, len(pred_signal))"
   ],
   "id": "8608121169d15e65",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "manifests[model_id]['loss_fn']",
   "id": "93b977e298d08b88",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the signals\n",
    "fig, ax = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "ax[0].plot(pred_signal, label='Prediction')\n",
    "ax[0].set_title('Prediction')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(gt_signal, label='Ground Truth')\n",
    "ax[1].set_title('Ground Truth')\n",
    "ax[1].legend()"
   ],
   "id": "6d659c47e668ad1e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.utils as utils\n",
    "import respiration.analysis as analysis\n",
    "\n",
    "compare = analysis.SignalCompare(\n",
    "    pred_signal,\n",
    "    sample_rate,\n",
    "    gt_signal,\n",
    "    sample_rate,\n",
    "    # normalize_signal=False,\n",
    "    # filter_signal=False,\n",
    "    # detrend_tarvainen=False,\n",
    ")\n",
    "\n",
    "utils.pretty_print(compare.distances())"
   ],
   "id": "aac87ba23dd1c359",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "compare_results = []\n",
    "\n",
    "for model_id, manifest in manifests.items():\n",
    "    print(model_id)\n",
    "\n",
    "    for (subject, setting) in manifest['testing_scenarios']:\n",
    "        pred_signal, sample_rate = get_prediction(subject, setting, model_id)\n",
    "        gt_signal = get_ground_truth(subject, setting, sample_rate, len(pred_signal))\n",
    "\n",
    "        compare = analysis.SignalCompare(\n",
    "            pred_signal,\n",
    "            sample_rate,\n",
    "            gt_signal,\n",
    "            sample_rate,\n",
    "        )\n",
    "\n",
    "        entry = compare.distances()\n",
    "        entry['model_id'] = model_id\n",
    "        entry['subject'] = subject\n",
    "        entry['setting'] = setting\n",
    "        compare_results.append(entry)\n",
    "\n",
    "compare_results = pd.DataFrame(compare_results)"
   ],
   "id": "f0391e364567af4e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "compare_results",
   "id": "81b36bd3dbff6ed3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_scores = []\n",
    "\n",
    "for model_id, manifest in manifests.items():\n",
    "    model_results = compare_results[compare_results['model_id'] == model_id]\n",
    "\n",
    "    model_scores.append({\n",
    "        'model_id': model_id,\n",
    "        'loss_fn': manifest['loss_fn'],\n",
    "        'optimizer': manifest['optimizer'],\n",
    "        'learning_rate': manifest['learning_rate'],\n",
    "        'chunk_size': manifest['chunk_size'],\n",
    "        'epochs': manifest['epochs'],\n",
    "        'mean_nfcp_error': model_results['nfcp_error'].mean(),\n",
    "        'mean_distance_dtw': model_results['distance_dtw'].mean(),\n",
    "        'mean_distance_mse': model_results['distance_mse'].mean(),\n",
    "        'mean_distance_pearson': model_results['distance_pearson'].mean(),\n",
    "    })\n",
    "\n",
    "model_scores = pd.DataFrame(model_scores)"
   ],
   "id": "b68f9f965724db3d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_scores",
   "id": "4caf84d84e132285",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
