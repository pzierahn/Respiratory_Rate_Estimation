{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Exploration\n",
    "\n",
    "This notebook demonstrates how to read a video file, extract frames, and display them using OpenCV and Matplotlib. It also shows how to detect faces in a frame using the Viola-Jones algorithm."
   ],
   "id": "349ba13172cd3774"
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "\n",
    "input_video_path = '/Volumes/Patrick/VitalCamSet/Proband05/101_natural_lighting/Logitech HD Pro Webcam C920.avi'\n",
    "video = cv2.VideoCapture(input_video_path)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4a4d6ee80f41e1d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Video properties",
   "id": "77f07b6f92634c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the video properties\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "duration = frame_count / fps\n",
    "\n",
    "print(f'FPS: {fps}')\n",
    "print(f'Frame count: {frame_count}')\n",
    "print(f'Frame: {frame_width}x{frame_height}')\n",
    "print(f'Duration: {duration:.2f} seconds')"
   ],
   "id": "d7894034d041f859",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Display random frames",
   "id": "e31a2710464b2da4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# Seed the random number generator for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "frame_inx1 = random.randint(0, frame_count)\n",
    "frame_inx2 = random.randint(0, frame_count)\n",
    "\n",
    "video.set(cv2.CAP_PROP_POS_FRAMES, frame_inx1)\n",
    "_, frame1 = video.read()\n",
    "\n",
    "video.set(cv2.CAP_PROP_POS_FRAMES, frame_inx2)\n",
    "_, frame2 = video.read()"
   ],
   "id": "5dfbe5b61af18c57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB)\n",
    "axes[0].imshow(frame1)\n",
    "axes[0].set_title(f'Frame {frame_inx1}')\n",
    "\n",
    "frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
    "axes[1].imshow(frame2)\n",
    "axes[1].set_title(f'Frame {frame_inx2}')"
   ],
   "id": "8e3e0b0729265182",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Frame difference",
   "id": "2eaee4e45b81985b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "frame_diff = cv2.absdiff(frame1, frame2)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(frame_diff)\n",
    "plt.title('Difference between frames')\n",
    "plt.show()"
   ],
   "id": "404a94f5c510289c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Face detection",
   "id": "936be7df034600a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Detect the face in the first frame with Viola Jones algorithm\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "gray_frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray_frame1, 1.3, 5)\n",
    "\n",
    "face_detection = frame1.copy()\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(face_detection, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(face_detection)\n",
    "plt.title('Face detection')\n",
    "plt.show()"
   ],
   "id": "6fcfbf6ce401fe45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## YOLO Algorithm",
   "id": "c5b8843a59c08abe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "yolo_dir = os.path.join(os.getcwd(), '..', 'data', 'yolo')\n",
    "\n",
    "# Load the class labels\n",
    "with open(yolo_dir + '/yolov3.txt', 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Generate random colors for each class \n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "scale = 0.00392\n",
    "# blob = cv2.dnn.blobFromImage(frame1, scale, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "# Make a three channel image from the gray frame\n",
    "gray_xxx = cv2.merge((gray_frame1, gray_frame1, gray_frame1))\n",
    "blob = cv2.dnn.blobFromImage(gray_xxx, scale, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "net = cv2.dnn.readNet(yolo_dir + '/yolov3.weights', yolo_dir + '/yolov3.cfg')\n",
    "net.setInput(blob)"
   ],
   "id": "f352e4e791696132",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "blob.shape",
   "id": "8a315d7cd8119de5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gray_frame1.shape, gray_xxx.shape",
   "id": "803d76df7b58a571",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "layer_names = net.getLayerNames()\n",
    "unconnected_layers = net.getUnconnectedOutLayers()\n",
    "output_layers = [layer_names[inx - 1] for inx in unconnected_layers]\n",
    "output_layers"
   ],
   "id": "72a6f5e369adc831",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# run inference through the network\n",
    "# and gather predictions from output layers\n",
    "outs = net.forward(output_layers)"
   ],
   "id": "6791fb84c9c525d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# initialization\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "\n",
    "width, height = frame1.shape[1], frame1.shape[0]\n",
    "\n",
    "# for each detetion from each output layer \n",
    "# get the confidence, class id, bounding box params\n",
    "# and ignore weak detections (confidence < 0.5)\n",
    "for out in outs:\n",
    "    print('out.shape', out.shape)\n",
    "    for detection in out:\n",
    "        # The first 4 elements are the bounding box dimensions\n",
    "        box = detection[:5]\n",
    "\n",
    "        # The rest are the class probabilities\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "\n",
    "        if confidence >= 0.5:\n",
    "            center_x = int(box[0] * width)\n",
    "            center_y = int(box[1] * height)\n",
    "            w = int(box[2] * width)\n",
    "            h = int(box[3] * height)\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "\n",
    "            class_ids.append(class_id)\n",
    "            confidences.append(float(confidence))\n",
    "            boxes.append([x, y, w, h])"
   ],
   "id": "d183e7e54dc84b8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# A minimum confidence score below which detections are discarded entirely.\n",
    "score_threshold = 0.5\n",
    "# A non-maxima suppression threshold to filter overlapping and low-confidence boxes.\n",
    "nms_threshold = 0.4\n",
    "\n",
    "# Apply non-maximum suppression\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, score_threshold, nms_threshold)\n",
    "\n",
    "obj_image = frame1.copy()\n",
    "\n",
    "for inx in indices:\n",
    "    x, y, w, h = boxes[inx]\n",
    "\n",
    "    # Draw the bounding box on the frame\n",
    "    color = COLORS[class_ids[inx]]\n",
    "    cv2.rectangle(obj_image, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "    # Draw the class label\n",
    "    cv2.putText(obj_image, classes[class_ids[inx]], (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(obj_image)\n",
    "plt.title('YOLO Face detection')\n",
    "plt.show()"
   ],
   "id": "b786876a53043346",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiratory_extraction.utils.roi as roi\n",
    "\n",
    "yolo = roi.YOLO()\n",
    "objects = yolo.detect_classes(frame1, 'person')\n",
    "\n",
    "obj_image_2 = frame1.copy()\n",
    "\n",
    "for obj in objects:\n",
    "    box = obj\n",
    "    x, y, w, h = box\n",
    "\n",
    "    # Draw the bounding box on the frame\n",
    "    color = COLORS[0]\n",
    "    cv2.rectangle(obj_image_2, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(obj_image_2)\n",
    "plt.title('YOLO Face detection 2')\n",
    "plt.show()"
   ],
   "id": "a7d754fd68efe5b8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
