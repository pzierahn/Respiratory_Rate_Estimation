{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analyze Fine-Tuned EfficientPhys Models",
   "id": "156541bc5c985cd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the Predictions",
   "id": "f6fbc6544ceaedce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import respiration.utils as utils\n",
    "\n",
    "signals_path = utils.file_path('outputs', 'signals')\n",
    "predictions_path = utils.join_paths(signals_path, 'fine_tuned_predictions.csv')\n",
    "\n",
    "prediction = pd.read_csv(predictions_path)\n",
    "prediction['signal'] = prediction['signal'].apply(eval).apply(np.array)"
   ],
   "id": "36f66e8a71704a30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "prediction.head()",
   "id": "3c049d5de449d5a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Get the Ground Truth RR Signal",
   "id": "e925da72b92c6338"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ground_truth_file = utils.join_paths(signals_path, 'respiration_gt.csv')\n",
    "ground_truth = pd.read_csv(ground_truth_file)\n",
    "ground_truth['signal'] = ground_truth['signal'].apply(eval).apply(np.array)\n",
    "ground_truth.head()"
   ],
   "id": "aa5deabe3ffec84d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analyze the Performance of the Model",
   "id": "cda20c512d3a9b55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models = prediction['model'].unique()\n",
    "models"
   ],
   "id": "8608121169d15e65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The testing scenarios are the same for each model\n",
    "manifest_path = utils.file_path('models', 'fine_tuned', '20240509_122327', 'manifest.json')\n",
    "manifest = utils.read_json(manifest_path)\n",
    "testing_scenarios = manifest['testing_scenarios']\n",
    "testing_scenarios"
   ],
   "id": "fee54e5734b5e884",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import respiration.analysis as analysis\n",
    "\n",
    "analysis_dir = utils.dir_path('outputs', 'analysis', mkdir=True)\n",
    "analysis_file = os.path.join(analysis_dir, 'fine_tuned_analysis.csv')\n",
    "\n",
    "if os.path.exists(analysis_file):\n",
    "    analysis_results = pd.read_csv(analysis_file)\n",
    "else:\n",
    "    analysis_results = []\n",
    "\n",
    "    for model_id in tqdm(models):\n",
    "        for (subject, setting) in testing_scenarios:\n",
    "            scenario = prediction[(prediction['model'] == model_id) &\n",
    "                                  (prediction['subject'] == subject) &\n",
    "                                  (prediction['setting'] == setting)].iloc[0]\n",
    "            prediction_signal = scenario['signal']\n",
    "\n",
    "            scenario_gt = ground_truth[(ground_truth['subject'] == subject) &\n",
    "                                       (ground_truth['setting'] == setting)].iloc[0]\n",
    "\n",
    "            # Cut the signal to the same length\n",
    "            gt_signal = scenario_gt['signal'][1:len(prediction_signal) + 1]\n",
    "\n",
    "            comparator = analysis.SignalComparator(\n",
    "                prediction_signal,\n",
    "                gt_signal,\n",
    "                scenario['sampling_rate'],\n",
    "            )\n",
    "\n",
    "            for metric, result in comparator.all_results().items():\n",
    "                analysis_results.append({\n",
    "                    'model_id': model_id,\n",
    "                    'subject': subject,\n",
    "                    'setting': setting,\n",
    "                    'metric': metric,\n",
    "                    'prediction': result['prediction'],\n",
    "                    'ground_truth': result['ground_truth'],\n",
    "                })\n",
    "\n",
    "    analysis_results = pd.DataFrame(analysis_results)\n",
    "    analysis_results.to_csv(analysis_file, index=False)"
   ],
   "id": "f0391e364567af4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "analysis_results.head()",
   "id": "81b36bd3dbff6ed3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "analysis_results['error'] = (analysis_results['prediction'] - analysis_results['ground_truth']).abs()",
   "id": "28676123a8a7c73e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the pk metrics error for each model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = prediction['model'].unique()\n",
    "metrics = analysis_results['metric'].unique()\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "for model_id in models:\n",
    "    model_results = analysis_results[(analysis_results['model_id'] == model_id)]\n",
    "\n",
    "    # Plot the mean error for each model\n",
    "    mean_error = model_results['error'].mean()\n",
    "    plt.bar(model_id, mean_error, label=model_id)\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Mean Error')\n",
    "plt.title('Mean Error for Fine-Tuned Models')\n",
    "\n",
    "plt.show()"
   ],
   "id": "446b5e7658d3d393",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
