{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analyze Fine-Tuned EfficientPhys Models",
   "id": "156541bc5c985cd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the Predictions",
   "id": "f6fbc6544ceaedce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import respiration.utils as utils\n",
    "\n",
    "signals_path = utils.file_path('outputs', 'signals')\n",
    "predictions_path = utils.join_paths(signals_path, 'fine_tuned_predictions.csv')\n",
    "\n",
    "prediction = pd.read_csv(predictions_path)\n",
    "prediction['signal'] = prediction['signal'].apply(eval).apply(np.array)"
   ],
   "id": "36f66e8a71704a30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "prediction.head()",
   "id": "3c049d5de449d5a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Get the Ground Truth RR Signal",
   "id": "e925da72b92c6338"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ground_truth_file = utils.join_paths(signals_path, 'respiration_gt.csv')\n",
    "ground_truth = pd.read_csv(ground_truth_file)\n",
    "ground_truth['signal'] = ground_truth['signal'].apply(eval).apply(np.array)\n",
    "ground_truth.head()"
   ],
   "id": "aa5deabe3ffec84d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analyze the Performance of the Model",
   "id": "cda20c512d3a9b55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models = prediction['model'].unique()\n",
    "models"
   ],
   "id": "8608121169d15e65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import tqdm.auto as tqdm\n",
    "import respiration.analysis as analysis\n",
    "\n",
    "analysis_dir = utils.dir_path('outputs', 'analysis', mkdir=True)\n",
    "analysis_file = os.path.join(analysis_dir, 'fine_tuned_analysis.csv')\n",
    "\n",
    "if os.path.exists(analysis_file):\n",
    "    analysis_results = pd.read_csv(analysis_file)\n",
    "else:\n",
    "    analysis_results = []\n",
    "\n",
    "    for model_id, manifest in tqdm.tqdm(models.items()):\n",
    "        for (subject, setting) in manifest['testing_scenarios']:\n",
    "            scenario = prediction[(prediction['model'] == model_id) &\n",
    "                                  (prediction['subject'] == subject) &\n",
    "                                  (prediction['setting'] == setting)].iloc[0]\n",
    "            prediction_signal = scenario['signal']\n",
    "\n",
    "            scenario_gt = ground_truth[(ground_truth['subject'] == subject) &\n",
    "                                       (ground_truth['setting'] == setting)].iloc[0]\n",
    "\n",
    "            # Cut the signal to the same length\n",
    "            gt_signal = scenario_gt['signal'][:len(prediction_signal)]\n",
    "\n",
    "            compare = analysis.SignalCompare(\n",
    "                prediction_signal,\n",
    "                gt_signal,\n",
    "                scenario['sampling_rate'],\n",
    "            )\n",
    "\n",
    "            for distance_name, distance_value in compare.compare_all().items():\n",
    "                analysis_results.append({\n",
    "                    'model_id': model_id,\n",
    "                    'subject': subject,\n",
    "                    'setting': setting,\n",
    "                    'metric': distance_name,\n",
    "                    'result': distance_value,\n",
    "                })\n",
    "\n",
    "    analysis_results = pd.DataFrame(analysis_results)\n",
    "    analysis_results.to_csv(analysis_file, index=False)"
   ],
   "id": "f0391e364567af4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "analysis_results.head()",
   "id": "81b36bd3dbff6ed3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = [\n",
    "    'pk_error',\n",
    "    'cp_error',\n",
    "    'nfcp_error',\n",
    "    'psd_error',\n",
    "    'distance_mse',\n",
    "    'distance_pearson',\n",
    "    'distance_dtw',\n",
    "]\n",
    "metrics.sort()\n",
    "\n",
    "models = analysis_results['model_id'].unique()\n",
    "\n",
    "model_scores = []\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    data = analysis_results[analysis_results['metric'] == metric]\n",
    "\n",
    "    for model_id in models:\n",
    "        model_data = data[data['model_id'] == model_id]\n",
    "        mean = model_data['result'].mean()\n",
    "        std = model_data['result'].std()\n",
    "\n",
    "        model_scores.append({\n",
    "            'model_id': model_id,\n",
    "            'metric': metric,\n",
    "            'mean': mean,\n",
    "            'std': std,\n",
    "        })\n",
    "\n",
    "model_scores = pd.DataFrame(model_scores)\n",
    "model_scores.to_csv(utils.join_paths(analysis_dir, 'fine_tuned_scores.csv'), index=False)\n",
    "model_scores.head()"
   ],
   "id": "fbfd428ca4d58189",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(len(metrics), 1, figsize=(20, 20))\n",
    "fig.tight_layout(pad=5.0)\n",
    "fig.suptitle('Model Error Comparison')\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    data = model_scores[model_scores['metric'] == metric]\n",
    "\n",
    "    axes[idx].bar(data['model_id'], data['mean'], yerr=data['std'], capsize=5)\n",
    "    axes[idx].set_title(metric)\n",
    "    axes[idx].set_ylabel(metric)\n",
    "    axes[idx].set_xlabel('Model ID')\n",
    "    axes[idx].grid()\n",
    "\n",
    "# Store the plot as svg\n",
    "figure_dir = utils.dir_path('outputs', 'figures', 'fine_tuned', mkdir=True)\n",
    "utils.savefig(fig, figure_dir, 'model_error_comparison')"
   ],
   "id": "a1976d811f1f665a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
