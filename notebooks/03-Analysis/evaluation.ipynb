{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Combine analysis results",
   "id": "c94be0da1a891e6a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import respiration.utils as utils\n",
    "\n",
    "analysis_dir = utils.dir_path('outputs', 'analysis')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Harmonize the data\n",
    "\n",
    "All respiration extraction methods have slightly different data structures. We need to harmonize the data to be able to compare the models. Only the best performing method for each model is kept."
   ],
   "id": "9406c34f8aa86289"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unsupervised_path = utils.join_paths(analysis_dir, 'unsupervised_analysis.csv')\n",
    "\n",
    "unsupervised = pd.read_csv(unsupervised_path)\n",
    "\n",
    "# Only keep roi==chest, because it is the most accurate\n",
    "unsupervised = unsupervised[unsupervised['roi'] == 'chest']\n",
    "\n",
    "# Remove roi column\n",
    "unsupervised = unsupervised.drop(columns=['roi'])\n",
    "\n",
    "# Rename method to model\n",
    "unsupervised = unsupervised.rename(columns={'method': 'model'})"
   ],
   "id": "2d762b7b5ee40cdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fine_tuned_path = utils.join_paths(analysis_dir, 'fine_tuned_analysis.csv')\n",
    "\n",
    "fine_tuned = pd.read_csv(fine_tuned_path)\n",
    "# Rename model_id to model\n",
    "fine_tuned = fine_tuned.rename(columns={'model_id': 'model'})\n",
    "\n",
    "fine_tuned['model'] = 'fine_tuned_' + fine_tuned['model']"
   ],
   "id": "294bc453f628d9a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "transformer_path = utils.join_paths(analysis_dir, 'transformer_analysis.csv')\n",
    "\n",
    "transformer = pd.read_csv(transformer_path)\n",
    "# Rename model_id to model\n",
    "transformer = transformer.rename(columns={'model_id': 'model'})\n",
    "\n",
    "# Add to each model name the prefix 'transformer_'\n",
    "transformer['model'] = 'transformer_' + transformer['model']"
   ],
   "id": "638deb9c8f64488e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "raft_path = utils.join_paths(analysis_dir, 'raft_analysis.csv')\n",
    "raft = pd.read_csv(raft_path)\n",
    "\n",
    "# Only keep roi==chest, because it is the most accurate\n",
    "raft = raft[raft['roi'] == 'chest']\n",
    "raft = raft[raft['signal_direction'] == 'signal_v']\n",
    "\n",
    "# Remove roi and signal_direction columns\n",
    "raft = raft.drop(columns=['roi', 'signal_direction'])"
   ],
   "id": "f23a20f10bd5bb54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pretrained_path = utils.join_paths(analysis_dir, 'pretrained_analysis.csv')\n",
    "pretrained = pd.read_csv(pretrained_path)"
   ],
   "id": "7c47cda1b524911e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Merge all dataframes\n",
    "analysis = pd.concat([fine_tuned, pretrained, unsupervised, raft, transformer])"
   ],
   "id": "54badc2ce30e41ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compare all models",
   "id": "3ff5d94c779387f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "analysis['error'] = (analysis['prediction'] - analysis['ground_truth']).abs()",
   "id": "29d5695f8cbc33b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "analysis",
   "id": "804dbdea67a61535",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models = analysis['model'].unique()\n",
    "metrics = analysis['metric'].unique()"
   ],
   "id": "424da40efe2e10a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "correlations = []\n",
    "\n",
    "for model in models:\n",
    "    model_data = analysis[analysis['model'] == model]\n",
    "\n",
    "    for metric in metrics:\n",
    "        metric_data = model_data[model_data['metric'] == metric]\n",
    "\n",
    "        if len(metric_data) != 0:\n",
    "            correlation, p_value = stats.pearsonr(metric_data['prediction'], metric_data['ground_truth'])\n",
    "            rmse = ((metric_data['prediction'] - metric_data['ground_truth']) ** 2).mean() ** 0.5\n",
    "\n",
    "            correlations.append({\n",
    "                'model': model,\n",
    "                'metric': metric,\n",
    "                'correlation': correlation,\n",
    "                'p_value': p_value,\n",
    "                'rmse': rmse\n",
    "            })\n",
    "\n",
    "correlations = pd.DataFrame(correlations)\n",
    "correlations"
   ],
   "id": "79642d76ce0b4141",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Only show correlations that are significant\n",
    "correlations[correlations['p_value'] < 0.05]"
   ],
   "id": "cb7f60e5be4bde2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(models)",
   "id": "3d811e9da4806db8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualize the results",
   "id": "29ab1e910b11f83c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "figure_dir = utils.dir_path('outputs', 'figures')",
   "id": "78181b0cccfd5f9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plot the prediction vs ground truth",
   "id": "6a16a7a6f7088fe1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import respiration.utils as utils\n",
    "\n",
    "metric = 'pk'\n",
    "\n",
    "fig, axs = plt.subplots(3, 5, figsize=(20, 12))\n",
    "\n",
    "# Add some space between the plots\n",
    "fig.tight_layout(pad=5.0)\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    ax = axs[idx // 5, idx % 5]\n",
    "    model_data = analysis[(analysis['model'] == model) & (analysis['metric'] == metric)]\n",
    "\n",
    "    ax.scatter(model_data['prediction'], model_data['ground_truth'])\n",
    "\n",
    "    # Add a regression line\n",
    "    x = model_data['prediction']\n",
    "    y = model_data['ground_truth']\n",
    "    m, b = np.polyfit(x, y, 1)\n",
    "    ax.plot(x, m * x + b, color='red')\n",
    "\n",
    "    ax.set_xlabel('Prediction')\n",
    "    ax.set_ylabel('Ground truth')\n",
    "    ax.set_title(model)\n",
    "\n",
    "utils.savefig(fig, figure_dir, 'model_correlations')"
   ],
   "id": "833c234528eee03e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Bland-Altman plot",
   "id": "ce585f6a7a3da3e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axs = plt.subplots(3, 5, figsize=(20, 12))\n",
    "\n",
    "# Add some space between the plots\n",
    "fig.tight_layout(pad=5.0)\n",
    "\n",
    "# Bland-Altman plot, where the numbers from top to bottom are mean + 1.96 std., mean, and mean - 1.96 std., respectively.\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    ax = axs[idx // 5, idx % 5]\n",
    "    model_data = analysis[(analysis['model'] == model) & (analysis['metric'] == metric)]\n",
    "\n",
    "    mean = model_data['error'].mean()\n",
    "    std = model_data['error'].std()\n",
    "\n",
    "    ax.scatter(model_data['prediction'], model_data['error'])\n",
    "    ax.axhline(mean + 1.96 * std, color='red', linestyle='--')\n",
    "    ax.axhline(mean, color='red')\n",
    "    ax.axhline(mean - 1.96 * std, color='red', linestyle='--')\n",
    "\n",
    "    # Set the y range to be between -10 and 10\n",
    "    ax.set_ylim(-5, 12)\n",
    "\n",
    "    ax.set_xlabel('Prediction')\n",
    "    ax.set_ylabel('Error')\n",
    "    ax.set_title(model)\n",
    "\n",
    "utils.savefig(fig, figure_dir, 'bland_altman')"
   ],
   "id": "9701c1294ebb486a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot the RMSE",
   "id": "b4fbbd33588e2406"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "\n",
    "for model in models:\n",
    "    model_data = correlations[(correlations['model'] == model) &\n",
    "                              (correlations['metric'] == metric)]\n",
    "    ax.bar(model, model_data['rmse'].values[0])\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('RMSE of the models')\n",
    "\n",
    "# Rotate the x labels\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "utils.savefig(fig, figure_dir, 'rmse')"
   ],
   "id": "947792b58e8ecc86",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
