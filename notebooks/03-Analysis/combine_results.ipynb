{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Combine analysis results",
   "id": "c94be0da1a891e6a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import respiration.utils as utils\n",
    "\n",
    "analysis_dir = utils.dir_path('outputs', 'analysis')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Harmonize the data\n",
    "\n",
    "All respiration extraction methods have slightly different data structures. We need to harmonize the data to be able to compare the models. Only the best performing method for each model is kept."
   ],
   "id": "9406c34f8aa86289"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unsupervised_path = utils.join_paths(analysis_dir, 'unsupervised_analysis.csv')\n",
    "\n",
    "unsupervised = pd.read_csv(unsupervised_path)\n",
    "\n",
    "# Only keep roi==chest, because it is the most accurate\n",
    "unsupervised = unsupervised[unsupervised['roi'] == 'chest']\n",
    "\n",
    "# Remove roi column\n",
    "unsupervised = unsupervised.drop(columns=['roi'])\n",
    "\n",
    "# Rename method to model\n",
    "unsupervised = unsupervised.rename(columns={'method': 'model'})"
   ],
   "id": "2d762b7b5ee40cdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fine_tuned_path = utils.join_paths(analysis_dir, 'fine_tuned_analysis.csv')\n",
    "\n",
    "fine_tuned = pd.read_csv(fine_tuned_path)\n",
    "# Rename model_id to model\n",
    "fine_tuned = fine_tuned.rename(columns={'model_id': 'model'})"
   ],
   "id": "294bc453f628d9a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "raft_path = utils.join_paths(analysis_dir, 'raft_analysis.csv')\n",
    "raft = pd.read_csv(raft_path)\n",
    "\n",
    "# Only keep roi==chest, because it is the most accurate\n",
    "raft = raft[raft['roi'] == 'chest']\n",
    "raft = raft[raft['signal_direction'] == 'signal_v']\n",
    "\n",
    "# Remove roi column\n",
    "raft = raft.drop(columns=['roi'])"
   ],
   "id": "f23a20f10bd5bb54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pretrained_path = utils.join_paths(analysis_dir, 'pretrained_analysis.csv')\n",
    "pretrained = pd.read_csv(pretrained_path)"
   ],
   "id": "7c47cda1b524911e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Merge all dataframes\n",
    "analysis = pd.concat([fine_tuned, pretrained, unsupervised, raft])"
   ],
   "id": "54badc2ce30e41ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compare all models",
   "id": "3ff5d94c779387f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "analysis['error'] = (analysis['prediction'] - analysis['ground_truth']).abs()",
   "id": "29d5695f8cbc33b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "analysis",
   "id": "804dbdea67a61535",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models = analysis['model'].unique()\n",
    "metrics = analysis['metric'].unique()"
   ],
   "id": "424da40efe2e10a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "correlations = []\n",
    "\n",
    "for model in models:\n",
    "    model_data = analysis[analysis['model'] == model]\n",
    "\n",
    "    for metric in metrics:\n",
    "        metric_data = model_data[model_data['metric'] == metric]\n",
    "\n",
    "        if len(metric_data) != 0:\n",
    "            correlation, p_value = stats.pearsonr(metric_data['prediction'], metric_data['ground_truth'])\n",
    "\n",
    "            correlations.append({\n",
    "                'model': model,\n",
    "                'metric': metric,\n",
    "                'correlation': correlation,\n",
    "                'p_value': p_value,\n",
    "            })\n",
    "\n",
    "correlations = pd.DataFrame(correlations)\n",
    "correlations"
   ],
   "id": "79642d76ce0b4141",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the Ground Truth vs Prediction for each model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(len(models[:3]), 1, figsize=(20, 20))\n",
    "\n",
    "# Add some space between the plots\n",
    "fig.tight_layout(pad=10.0)\n",
    "\n",
    "fig.suptitle('Bland-Altman plots')\n",
    "\n",
    "for idx, model in enumerate(models[:3]):\n",
    "    ax = axes[idx]\n",
    "    model_data = analysis[analysis['model'] == model]\n",
    "\n",
    "    for metric in metrics:\n",
    "        metric_data = model_data[model_data['metric'] == metric]\n",
    "\n",
    "        if len(metric_data) != 0:\n",
    "            ax.scatter(metric_data['prediction'], metric_data['error'], label=metric)\n",
    "\n",
    "    ax.set_title(model)\n",
    "    ax.set_xlabel('Prediction')\n",
    "    ax.set_ylabel('Error')\n",
    "\n",
    "    ax.legend()"
   ],
   "id": "120aa0d0ae486554",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_scores = []\n",
    "\n",
    "for model in analysis['model'].unique():\n",
    "    model_data = analysis[analysis['model'] == model]\n",
    "\n",
    "    for metric in metrics:\n",
    "        metric_data = model_data[model_data['metric'] == metric]\n",
    "        if len(metric_data) != 0:\n",
    "            model_scores.append({\n",
    "                'model': model,\n",
    "                'metric': metric,\n",
    "                'mean': metric_data['error'].mean(),\n",
    "                'std': metric_data['error'].std(),\n",
    "            })\n",
    "\n",
    "model_scores = pd.DataFrame(model_scores)\n",
    "model_scores"
   ],
   "id": "4085ef53254ae319",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Store the model scores\n",
    "output_path = utils.dir_path('outputs', 'analysis', mkdir=True)\n",
    "model_scores.to_csv(utils.join_paths(output_path, 'model_scores.csv'), index=False)"
   ],
   "id": "5c09b24f45dd3bb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Determine the best model for each metric\n",
    "best_models = []\n",
    "\n",
    "for metric in metrics:\n",
    "    metric_data = model_scores[model_scores['metric'] == metric]\n",
    "\n",
    "    if metric == \"distance_pearson\":\n",
    "        value = metric_data['mean'].max()\n",
    "    else:\n",
    "        value = metric_data['mean'].min()\n",
    "\n",
    "    best_model = metric_data[metric_data['mean'] == value]\n",
    "    best_models.append(best_model)\n",
    "\n",
    "best_models = pd.concat(best_models)\n",
    "best_models"
   ],
   "id": "f6e933a136d5acdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(len(metrics), 1, figsize=(20, 20))\n",
    "\n",
    "# Add some space between the plots\n",
    "fig.tight_layout(pad=10.0)\n",
    "\n",
    "# Add some space between the plots\n",
    "fig.tight_layout(pad=5.0)\n",
    "\n",
    "fig.suptitle('Comparison of models')\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    scores = model_scores[model_scores['metric'] == metric]\n",
    "\n",
    "    for _, data in scores.iterrows():\n",
    "        ax.bar(data['model'], data['mean'], yerr=data['std'], capsize=5)\n",
    "\n",
    "    ax.set_title(metric)\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_xlabel('Model')\n",
    "\n",
    "    # Rotate the x-axis labels\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(0)\n",
    "\n",
    "# Store the plot as svg\n",
    "figure_dir = utils.dir_path('outputs', 'figures', mkdir=True)\n",
    "utils.savefig(fig, figure_dir, 'model_comparison')"
   ],
   "id": "2f346dfac4bc9270",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
