{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Extract respiratory signals with FlowNet2 optical flow\n",
    "\n",
    "Based on \"FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks\" is a deep learning model for optical flow estimation. The optical flow directions and magnitudes can be used to extract respiratory signals from videos. This notebook demonstrates how to use RAFT to extract respiratory signals from videos."
   ],
   "id": "e931c39f16bae87e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from respiration.dataset import VitalCamSet\n",
    "\n",
    "dataset = VitalCamSet()\n",
    "\n",
    "subject = 'Proband16'\n",
    "setting = '101_natural_lighting'\n",
    "\n",
    "video_path = dataset.get_video_path(subject, setting)"
   ],
   "id": "3f5527a9bf000",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define FlowNet 2.0 model",
   "id": "5b39d80a238d40de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1):\n",
    "    if batchNorm:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.LeakyReLU(0.1, inplace=True)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2,\n",
    "                      bias=True),\n",
    "            nn.LeakyReLU(0.1, inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "def i_conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, bias=True):\n",
    "    if batchNorm:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2,\n",
    "                      bias=bias),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2,\n",
    "                      bias=bias),\n",
    "        )\n",
    "\n",
    "\n",
    "def predict_flow(in_planes):\n",
    "    return nn.Conv2d(in_planes, 2, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "\n",
    "\n",
    "def deconv(in_planes, out_planes):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_planes, out_planes, kernel_size=4, stride=2, padding=1, bias=True),\n",
    "        nn.LeakyReLU(0.1, inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class tofp16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(tofp16, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.half()\n",
    "\n",
    "\n",
    "class tofp32(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(tofp32, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.float()\n",
    "\n",
    "\n",
    "def init_deconv_bilinear(weight):\n",
    "    f_shape = weight.size()\n",
    "    heigh, width = f_shape[-2], f_shape[-1]\n",
    "    f = np.ceil(width / 2.0)\n",
    "    c = (2 * f - 1 - f % 2) / (2.0 * f)\n",
    "    bilinear = np.zeros([heigh, width])\n",
    "    for x in range(width):\n",
    "        for y in range(heigh):\n",
    "            value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n",
    "            bilinear[x, y] = value\n",
    "    weight.data.fill_(0.)\n",
    "    for i in range(f_shape[0]):\n",
    "        for j in range(f_shape[1]):\n",
    "            weight.data[i, j, :, :] = torch.from_numpy(bilinear)\n",
    "\n",
    "\n",
    "def save_grad(grads, name):\n",
    "    def hook(grad):\n",
    "        grads[name] = grad\n",
    "\n",
    "    return hook"
   ],
   "id": "f3777f29e3423432",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "class FlowNetSD(nn.Module):\n",
    "    def __init__(self, batchNorm=True):\n",
    "        super(FlowNetSD, self).__init__()\n",
    "\n",
    "        self.batchNorm = batchNorm\n",
    "        self.conv0 = conv(self.batchNorm, 6, 64)\n",
    "        self.conv1 = conv(self.batchNorm, 64, 64, stride=2)\n",
    "        self.conv1_1 = conv(self.batchNorm, 64, 128)\n",
    "        self.conv2 = conv(self.batchNorm, 128, 128, stride=2)\n",
    "        self.conv2_1 = conv(self.batchNorm, 128, 128)\n",
    "        self.conv3 = conv(self.batchNorm, 128, 256, stride=2)\n",
    "        self.conv3_1 = conv(self.batchNorm, 256, 256)\n",
    "        self.conv4 = conv(self.batchNorm, 256, 512, stride=2)\n",
    "        self.conv4_1 = conv(self.batchNorm, 512, 512)\n",
    "        self.conv5 = conv(self.batchNorm, 512, 512, stride=2)\n",
    "        self.conv5_1 = conv(self.batchNorm, 512, 512)\n",
    "        self.conv6 = conv(self.batchNorm, 512, 1024, stride=2)\n",
    "        self.conv6_1 = conv(self.batchNorm, 1024, 1024)\n",
    "\n",
    "        self.deconv5 = deconv(1024, 512)\n",
    "        self.deconv4 = deconv(1026, 256)\n",
    "        self.deconv3 = deconv(770, 128)\n",
    "        self.deconv2 = deconv(386, 64)\n",
    "\n",
    "        self.inter_conv5 = i_conv(self.batchNorm, 1026, 512)\n",
    "        self.inter_conv4 = i_conv(self.batchNorm, 770, 256)\n",
    "        self.inter_conv3 = i_conv(self.batchNorm, 386, 128)\n",
    "        self.inter_conv2 = i_conv(self.batchNorm, 194, 64)\n",
    "\n",
    "        self.predict_flow6 = predict_flow(1024)\n",
    "        self.predict_flow5 = predict_flow(512)\n",
    "        self.predict_flow4 = predict_flow(256)\n",
    "        self.predict_flow3 = predict_flow(128)\n",
    "        self.predict_flow2 = predict_flow(64)\n",
    "\n",
    "        self.upsampled_flow6_to_5 = nn.ConvTranspose2d(2, 2, 4, 2, 1)\n",
    "        self.upsampled_flow5_to_4 = nn.ConvTranspose2d(2, 2, 4, 2, 1)\n",
    "        self.upsampled_flow4_to_3 = nn.ConvTranspose2d(2, 2, 4, 2, 1)\n",
    "        self.upsampled_flow3_to_2 = nn.ConvTranspose2d(2, 2, 4, 2, 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m.bias is not None:\n",
    "                    init.uniform_(m.bias)\n",
    "                init.xavier_uniform_(m.weight)\n",
    "\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                if m.bias is not None:\n",
    "                    init.uniform_(m.bias)\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                # init_deconv_bilinear(m.weight)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=4, mode='bilinear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_conv0 = self.conv0(x)\n",
    "        out_conv1 = self.conv1_1(self.conv1(out_conv0))\n",
    "        out_conv2 = self.conv2_1(self.conv2(out_conv1))\n",
    "\n",
    "        out_conv3 = self.conv3_1(self.conv3(out_conv2))\n",
    "        out_conv4 = self.conv4_1(self.conv4(out_conv3))\n",
    "        out_conv5 = self.conv5_1(self.conv5(out_conv4))\n",
    "        out_conv6 = self.conv6_1(self.conv6(out_conv5))\n",
    "\n",
    "        flow6 = self.predict_flow6(out_conv6)\n",
    "        flow6_up = self.upsampled_flow6_to_5(flow6)\n",
    "        out_deconv5 = self.deconv5(out_conv6)\n",
    "\n",
    "        concat5 = torch.cat((out_conv5, out_deconv5, flow6_up), 1)\n",
    "        out_interconv5 = self.inter_conv5(concat5)\n",
    "        flow5 = self.predict_flow5(out_interconv5)\n",
    "\n",
    "        flow5_up = self.upsampled_flow5_to_4(flow5)\n",
    "        out_deconv4 = self.deconv4(concat5)\n",
    "\n",
    "        concat4 = torch.cat((out_conv4, out_deconv4, flow5_up), 1)\n",
    "        out_interconv4 = self.inter_conv4(concat4)\n",
    "        flow4 = self.predict_flow4(out_interconv4)\n",
    "        flow4_up = self.upsampled_flow4_to_3(flow4)\n",
    "        out_deconv3 = self.deconv3(concat4)\n",
    "\n",
    "        concat3 = torch.cat((out_conv3, out_deconv3, flow4_up), 1)\n",
    "        out_interconv3 = self.inter_conv3(concat3)\n",
    "        flow3 = self.predict_flow3(out_interconv3)\n",
    "        flow3_up = self.upsampled_flow3_to_2(flow3)\n",
    "        out_deconv2 = self.deconv2(concat3)\n",
    "\n",
    "        concat2 = torch.cat((out_conv2, out_deconv2, flow3_up), 1)\n",
    "        out_interconv2 = self.inter_conv2(concat2)\n",
    "        flow2 = self.predict_flow2(out_interconv2)\n",
    "\n",
    "        if self.training:\n",
    "            return flow2, flow3, flow4, flow5, flow6\n",
    "        else:\n",
    "            return flow2,"
   ],
   "id": "74693477954cf1a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class FlowNet2SD(FlowNetSD):\n",
    "    def __init__(self, args: dict, batchNorm=False, div_flow=20):\n",
    "        super(FlowNet2SD, self).__init__(batchNorm=batchNorm)\n",
    "        self.rgb_max = args['rgb_max']\n",
    "        self.div_flow = div_flow\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        rgb_mean = inputs.contiguous().view(inputs.size()[:2] + (-1,)).mean(dim=-1).view(inputs.size()[:2] + (1, 1, 1,))\n",
    "        x = (inputs - rgb_mean) / self.rgb_max\n",
    "        x = torch.cat((x[:, :, 0, :, :], x[:, :, 1, :, :]), dim=1)\n",
    "\n",
    "        out_conv0 = self.conv0(x)\n",
    "        out_conv1 = self.conv1_1(self.conv1(out_conv0))\n",
    "        out_conv2 = self.conv2_1(self.conv2(out_conv1))\n",
    "\n",
    "        out_conv3 = self.conv3_1(self.conv3(out_conv2))\n",
    "        out_conv4 = self.conv4_1(self.conv4(out_conv3))\n",
    "        out_conv5 = self.conv5_1(self.conv5(out_conv4))\n",
    "        out_conv6 = self.conv6_1(self.conv6(out_conv5))\n",
    "\n",
    "        flow6 = self.predict_flow6(out_conv6)\n",
    "        flow6_up = self.upsampled_flow6_to_5(flow6)\n",
    "        out_deconv5 = self.deconv5(out_conv6)\n",
    "\n",
    "        concat5 = torch.cat((out_conv5, out_deconv5, flow6_up), 1)\n",
    "        out_interconv5 = self.inter_conv5(concat5)\n",
    "        flow5 = self.predict_flow5(out_interconv5)\n",
    "\n",
    "        flow5_up = self.upsampled_flow5_to_4(flow5)\n",
    "        out_deconv4 = self.deconv4(concat5)\n",
    "\n",
    "        concat4 = torch.cat((out_conv4, out_deconv4, flow5_up), 1)\n",
    "        out_interconv4 = self.inter_conv4(concat4)\n",
    "        flow4 = self.predict_flow4(out_interconv4)\n",
    "        flow4_up = self.upsampled_flow4_to_3(flow4)\n",
    "        out_deconv3 = self.deconv3(concat4)\n",
    "\n",
    "        concat3 = torch.cat((out_conv3, out_deconv3, flow4_up), 1)\n",
    "        out_interconv3 = self.inter_conv3(concat3)\n",
    "        flow3 = self.predict_flow3(out_interconv3)\n",
    "        flow3_up = self.upsampled_flow3_to_2(flow3)\n",
    "        out_deconv2 = self.deconv2(concat3)\n",
    "\n",
    "        concat2 = torch.cat((out_conv2, out_deconv2, flow3_up), 1)\n",
    "        out_interconv2 = self.inter_conv2(concat2)\n",
    "        flow2 = self.predict_flow2(out_interconv2)\n",
    "\n",
    "        if self.training:\n",
    "            return flow2, flow3, flow4, flow5, flow6\n",
    "        else:\n",
    "            return self.upsample1(flow2 * self.div_flow)"
   ],
   "id": "871d8f7576edcb64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the FlowNet model",
   "id": "8b708a15f7bc46fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import respiration.utils as utils\n",
    "\n",
    "device = utils.get_torch_device()\n",
    "path = utils.file_path('data', 'flownet', 'FlowNet2-SD_checkpoint.pth')\n",
    "loaded = torch.load(path)\n",
    "\n",
    "args = {\n",
    "    'rgb_max': 255,\n",
    "    'div_flow': 20,\n",
    "}\n",
    "model = FlowNet2SD(args, batchNorm=False)\n",
    "model.load_state_dict(loaded['state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()"
   ],
   "id": "c50c750fcb5657b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Extract optical flow from the video",
   "id": "b67acd67436fb434"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def resize_and_center_frames(frames: np.ndarray, target_size: (int, int)):\n",
    "    resized_frames = []\n",
    "\n",
    "    for frame in frames:\n",
    "        # Resize frame to target size while maintaining aspect ratio\n",
    "        height, width = frame.shape[:2]\n",
    "        aspect_ratio = width / height\n",
    "\n",
    "        if width > height:\n",
    "            new_width = target_size[0]\n",
    "            new_height = int(new_width / aspect_ratio)\n",
    "        else:\n",
    "            new_height = target_size[1]\n",
    "            new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "        resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "\n",
    "        # Create a new blank frame with target size\n",
    "        centered_frame = np.zeros((target_size[1], target_size[0], 3), dtype=np.uint8)\n",
    "\n",
    "        # Calculate top-left corner for centering the resized frame\n",
    "        x_offset = (target_size[0] - new_width) // 2\n",
    "        y_offset = (target_size[1] - new_height) // 2\n",
    "\n",
    "        # Place the resized frame in the center of the new blank frame\n",
    "        centered_frame[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized_frame\n",
    "\n",
    "        resized_frames.append(centered_frame)\n",
    "\n",
    "    return resized_frames"
   ],
   "id": "808ba1b80da819b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "import respiration.utils as utils\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "param = utils.get_video_params(video_path)\n",
    "\n",
    "# Only get the first 12 seconds of the video\n",
    "param.num_frames = param.fps * 30\n",
    "\n",
    "# Number of frames that are processed at once\n",
    "batch_size = 120\n",
    "batches = math.ceil(param.num_frames / batch_size)\n",
    "\n",
    "new_dim = 640\n",
    "\n",
    "# Store the optical flows vectors (N, 2, H, W)\n",
    "optical_flows = np.zeros((param.num_frames - 1, 2, new_dim, new_dim), dtype=np.float32)\n",
    "\n",
    "# Extract the optical flow from the video in batches\n",
    "for batch in tqdm(range(0, batches)):\n",
    "    start = batch_size * batch - batch\n",
    "    num_frames = min(batch_size, param.num_frames - start)\n",
    "    frames, _ = utils.read_video_rgb(video_path, num_frames, start)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(mode='RGB'),\n",
    "        # Center Crop the frames\n",
    "        transforms.CenterCrop((new_dim, new_dim)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    frames = torch.stack([preprocess(frame) for frame in frames], dim=0)\n",
    "    frames = frames.to(device)\n",
    "\n",
    "    # Fold the frames into (T, C, 2, H, W)\n",
    "    unfolded_frames = frames.unfold(0, 2, 1).permute(0, 1, 4, 2, 3)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        flows = model(unfolded_frames)\n",
    "\n",
    "    # Garbage collect...\n",
    "    del frames, unfolded_frames\n",
    "\n",
    "    for idx in range(flows.shape[0]):\n",
    "        # Add the optical flow to the numpy array\n",
    "        optical_flows[start + idx] = flows[idx].cpu().numpy()"
   ],
   "id": "ef2039ce2b562fa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualize the optical flow",
   "id": "21f8ecfff969c042"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "figure_dir = utils.dir_path('outputs', 'figures', 'flownet', mkdir=True)",
   "id": "73ab1e6b51bb90c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import respiration.extractor.optical_flow_raft as raft\n",
    "\n",
    "frames, _ = utils.read_video_rgb(video_path, 1, 1)\n",
    "frames = resize_and_center_frames(frames, (new_dim, new_dim))\n",
    "arrow_frame = raft.draw_flow(frames[0], optical_flows[0])\n",
    "flow_frame = raft.image_from_flow(optical_flows[0])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "ax[0].imshow(arrow_frame)\n",
    "ax[0].set_title('Optical flow arrows')\n",
    "ax[1].imshow(flow_frame)\n",
    "ax[1].set_title('Optical flow magnitude')\n",
    "\n",
    "utils.savefig(fig, figure_dir, 'optical_flow')"
   ],
   "id": "55a7216db8ee3b34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extract the respiratory signal\n",
    "\n",
    "1. Find the region of interest (ROI) on the chest\n",
    "2. Calculate the motion magnitude in the ROI\n",
    "3. Plot the motion magnitude over time"
   ],
   "id": "874aecbbb53b00ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.roi as roi\n",
    "\n",
    "# Find the chest region\n",
    "x, y, w, h = roi.detect_chest(frames[0])\n",
    "\n",
    "# Get only the optical flows in the chest region\n",
    "roi_flows = optical_flows[:, :, y:y + h, x:x + w]\n",
    "\n",
    "# Calculate motion magnitude by squaring the x and y components and taking the square root\n",
    "magnitudes = np.sqrt(roi_flows[:, 1] ** 2)"
   ],
   "id": "8aa5cd3d30abeb14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mean_curve = np.mean(magnitudes, axis=(1, 2))\n",
    "std_curve = np.std(magnitudes, axis=(1, 2))"
   ],
   "id": "8d213c646c6e4318",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20, 8))\n",
    "ax.plot(mean_curve, label='Mean')\n",
    "ax.fill_between(\n",
    "    np.arange(len(mean_curve)),\n",
    "    mean_curve - std_curve,\n",
    "    mean_curve + std_curve,\n",
    "    alpha=0.3,\n",
    "    label='Standard deviation')\n",
    "\n",
    "ax.set_title('Motion magnitude in the ROI')\n",
    "ax.set_xlabel('Frame')\n",
    "ax.set_ylabel('Motion magnitude')\n",
    "\n",
    "utils.savefig(fig, figure_dir, 'roi_magnitudes')"
   ],
   "id": "3f348a0c6c9a6316",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.analysis as analysis\n",
    "\n",
    "respiratory_gt = dataset.get_breathing_signal(subject, setting)[1:param.num_frames]\n",
    "\n",
    "comparator = analysis.SignalComparator(respiratory_gt, mean_curve, sample_rate=param.fps)\n",
    "utils.pretty_print(comparator.errors())"
   ],
   "id": "c22b159aeeca2805",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20, 8))\n",
    "ax.plot(comparator.prediction, label='Prediction')\n",
    "ax.plot(comparator.ground_truth, label='Ground truth')\n",
    "ax.set_title('Respiratory signal')\n",
    "ax.set_xlabel('Frame')\n",
    "ax.set_ylabel('Motion magnitude')\n",
    "ax.legend()"
   ],
   "id": "1b8b4fcc54ac5dad",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
