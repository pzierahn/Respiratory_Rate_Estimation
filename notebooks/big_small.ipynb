{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.dataset as repository\n",
    "\n",
    "dataset = repository.from_default()\n",
    "\n",
    "subject = 'Proband16'\n",
    "scenario = '101_natural_lighting'"
   ],
   "id": "4b78c47457679ff0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "frames, meta = dataset.get_video_bgr(subject, scenario)",
   "id": "a61011c940742495",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    # Use the MPS (Multi-Process Service) to run the model\n",
    "    # This is only available on macOS\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    # Use the GPU to run the model\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    # Use the CPU to run the model\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ],
   "id": "42faa5548dc370fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from respiration.extractor.r_ppg.big_small import BigSmall\n",
    "\n",
    "model_path = '../data/deep_phys/BP4D_BigSmall_Multitask_Fold3.pth'\n",
    "\n",
    "# Wrap modul in nn.DataParallel\n",
    "model = BigSmall()\n",
    "# Fix model loading: Some key have an extra 'module.' prefix\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "# Load the model with the weights\n",
    "key_matching = model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "key_matching"
   ],
   "id": "c4dc5f528d1e36c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.eval()",
   "id": "2b6a2d6c736de2a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocess_frames(frames: np.array, big_res=144, small_res=9):\n",
    "    # Center crop frames to square shape\n",
    "    h, w, _ = frames[0].shape\n",
    "    crop_size = min(h, w)\n",
    "    start_y = (h - crop_size) // 2\n",
    "    start_x = (w - crop_size) // 2\n",
    "    frames = [frame[start_y:start_y + crop_size, start_x:start_x + crop_size] for frame in frames]\n",
    "\n",
    "    # Convert frames to floating point\n",
    "    frames = np.array(frames, dtype=np.float32)\n",
    "\n",
    "    # Generate Small branch inputs (normalized difference frames)\n",
    "    diff_frames = frames[1:] - frames[:-1]\n",
    "    sum_frames = frames[1:] + frames[:-1]\n",
    "    small_inputs = diff_frames / (sum_frames + 1e-7)\n",
    "    small_inputs = (small_inputs - np.mean(small_inputs)) / np.std(small_inputs)\n",
    "    small_inputs = [cv2.resize(frame, (small_res, small_res)) for frame in small_inputs]\n",
    "\n",
    "    # Fix missing first frame\n",
    "    small_inputs = small_inputs + [np.zeros_like(small_inputs[0])]\n",
    "\n",
    "    # Generate Big branch inputs (raw frames)\n",
    "    big_inputs = (frames - np.mean(frames)) / np.std(frames)\n",
    "    big_inputs = [cv2.resize(frame, (big_res, big_res)) for frame in big_inputs]\n",
    "\n",
    "    return small_inputs, big_inputs"
   ],
   "id": "61a8ad257b2a406f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the first 10 seconds of the video\n",
    "from respiration.utils import video\n",
    "\n",
    "rgb_frames = video.bgr_to_rgb(frames[:300])\n",
    "small, big = preprocess_frames(rgb_frames, big_res=144, small_res=9)"
   ],
   "id": "c9cfc5b066807c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Show the first big and small frame\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axs[0].imshow(small[0])\n",
    "axs[0].set_title('Small Frame')\n",
    "\n",
    "axs[1].imshow(big[0])\n",
    "axs[1].set_title('Big Frame')"
   ],
   "id": "798792a41bcdf4f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert the frames to a tensor\n",
    "small_tensor = torch.tensor(np.array(small), device=device)\n",
    "big_tensor = torch.tensor(np.array(big), device=device)"
   ],
   "id": "764f8da473b14cfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "small_tensor.shape, big_tensor.shape",
   "id": "573e06d38c7fe08c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Transform the tensor to the shape expected by the model (frame_count, c, w, h)\n",
    "small_tensor = small_tensor.permute(0, 3, 1, 2)\n",
    "big_tensor = big_tensor.permute(0, 3, 1, 2)\n",
    "\n",
    "small_tensor.shape, big_tensor.shape"
   ],
   "id": "f6e72af9225b6bed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract the signals\n",
    "with torch.no_grad():\n",
    "    au_out, bvp_out, resp_out = model((big_tensor, small_tensor))\n",
    "\n",
    "resp_out.shape"
   ],
   "id": "f4da6f5049773234",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.preprocessing as preprocessing\n",
    "\n",
    "waveform = resp_out.cpu().numpy().squeeze()\n",
    "waveform = preprocessing.detrend_tarvainen(waveform)\n",
    "waveform = preprocessing.butterworth_filter(waveform, meta.fps, 0.8, 3.0)"
   ],
   "id": "6ea84aca0bd934a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "waveform.shape",
   "id": "48877a36a2645e5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the rPPG signal\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(waveform)\n",
    "plt.title('Respiration Signal')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ],
   "id": "ef4da6e782ccaae9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "chunk_size = 300\n",
    "\n",
    "waveform = None\n",
    "\n",
    "for inx in tqdm(range(0, len(frames), chunk_size)):\n",
    "    end = min(inx + chunk_size, len(frames))\n",
    "    rgb_frames = video.bgr_to_rgb(frames[inx:end])\n",
    "    small, big = preprocess_frames(rgb_frames, big_res=144, small_res=9)\n",
    "\n",
    "    # Convert the frames to a tensor\n",
    "    small_tensor = torch.tensor(small, device=device)\n",
    "    big_tensor = torch.tensor(big, device=device)\n",
    "\n",
    "    # Transform the tensor to the shape expected by the model (frame_count, c, w, h)\n",
    "    small_tensor = small_tensor.permute(0, 3, 1, 2)\n",
    "    big_tensor = big_tensor.permute(0, 3, 1, 2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, _, resp_out = model((big_tensor, small_tensor))\n",
    "        \n",
    "        if waveform is None:\n",
    "            waveform = resp_out.cpu().numpy()\n",
    "        else:\n",
    "            waveform = np.concatenate((waveform, resp_out.cpu().numpy()), axis=0)\n",
    "\n",
    "waveform = waveform.squeeze()"
   ],
   "id": "250a86a81bcf047e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "waveform_processed = preprocessing.detrend_tarvainen(waveform)\n",
    "waveform_processed = preprocessing.butterworth_filter(waveform_processed, meta.fps, 0.08, 0.6)"
   ],
   "id": "1b1ab5da1ce3c527",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the signals\n",
    "_, axs = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "axs[0].plot(waveform)\n",
    "axs[0].set_title('Signal')\n",
    "axs[0].set_xlabel('Frame')\n",
    "axs[0].set_ylabel('Amplitude')\n",
    "\n",
    "axs[1].plot(waveform_processed)\n",
    "axs[1].set_title('Processed')\n",
    "axs[1].set_xlabel('Frame')\n",
    "axs[1].set_ylabel('Amplitude')"
   ],
   "id": "84a82637ac3ef49d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
